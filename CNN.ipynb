{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "HhoE0veRfHoT"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "eYhLQOn3YqGA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#**Convolutional Neural Network**\n",
        "The problem we are trying to solve here is to classify grayscale images of handwritten objects (28 pixels by 28 pixels), into 10 categories (apple, banana, fork...). \n",
        "\n",
        "The dataset we will use is extracted from the Kaggle competition: **Quick Draw! Doodle Recognition Challenge ** (https://www.kaggle.com/c/quickdraw-doodle-recognition). This dataset contains "
      ]
    },
    {
      "metadata": {
        "id": "dwTJ68cmcdlr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#**1. Notebook Setting**\n",
        "\n",
        "Import Pytorch and Python libraries (Numpy, Matplotlib...)"
      ]
    },
    {
      "metadata": {
        "id": "b_k3w0d1hxzB",
        "colab_type": "code",
        "outputId": "c6bf4be2-c157-44bd-97c7-d2fe6235bafc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "  \n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torchvision\n",
        "import random\n",
        "import codecs\n",
        "import torch.utils.data\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as datasets\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "print('Done!')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ADImzkBIqHd_",
        "colab_type": "code",
        "outputId": "84207360-699c-474e-9b66-c80f5bdf601c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#Training on the GPU\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1euniB3GOHmk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **2. Dataset Preparation**\n",
        "\n",
        "Download, reduce, reshape and reorganize dataset\n"
      ]
    },
    {
      "metadata": {
        "id": "E1ww9hWadKaR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **2.1 Download the Dataset:**\n",
        "\n",
        "The dataset is downloaded from the Google APIs and it comes in the form of a set of Numpy arrays."
      ]
    },
    {
      "metadata": {
        "id": "cfmjdhj88dfN",
        "colab_type": "code",
        "outputId": "90646f0b-e523-499f-86a0-70527b608e07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "  urls = [\n",
        "        'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/key.npy',\n",
        "        'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/banana.npy',\n",
        "        'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/ladder.npy',\n",
        "        'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/tennis%20racquet.npy',\n",
        "        'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/pizza.npy',\n",
        "        'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/stop%20sign.npy',\n",
        "        'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/wheel.npy',\n",
        "        'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/fork.npy',\n",
        "        'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/book.npy',\n",
        "        'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/apple.npy',\n",
        "    ]\n",
        "  \n",
        "  class_name = ['key', 'banana', 'ladder', 'tennis_racquet', 'pizza', 'stop_sign', 'wheel', 'fork', 'book', 'apple']\n",
        "   \n",
        "  def createDir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    \n",
        "  def gen_bar_updater(pbar):\n",
        "    def bar_update(count, block_size, total_size):\n",
        "        if pbar.total is None and total_size:\n",
        "            pbar.total = total_size\n",
        "        progress_bytes = count * block_size\n",
        "        pbar.update(progress_bytes - pbar.n)\n",
        "    return bar_update   \n",
        "    \n",
        "  def download_url(url, root, filename):\n",
        "      from six.moves import urllib\n",
        "      root = os.path.expanduser(root)\n",
        "      fpath = os.path.join(root, filename + \".npy\")\n",
        "\n",
        "      createDir(root)\n",
        "\n",
        "      # downloads file\n",
        "      if os.path.isfile(fpath):\n",
        "          a = 1\n",
        "          #print('Using downloaded and verified file: ' + fpath)\n",
        "      else:\n",
        "          try:\n",
        "              print('Downloading ' + url + ' to ' + fpath)\n",
        "              urllib.request.urlretrieve(\n",
        "                  url, fpath,\n",
        "                  reporthook = gen_bar_updater(tqdm(unit='B', unit_scale=True))\n",
        "              )\n",
        "          except OSError:\n",
        "              if url[:5] == 'https':\n",
        "                  url = url.replace('https:', 'http:')\n",
        "                  print('Failed download. Trying https -> http instead.'\n",
        "                        ' Downloading ' + url + ' to ' + fpath)\n",
        "                  urllib.request.urlretrieve(\n",
        "                      url, fpath,\n",
        "                      reporthook = gen_bar_updater(tqdm(unit='B', unit_scale=True))\n",
        "                  )\n",
        "                  \n",
        "                  \n",
        "                  \n",
        "  for i in range(0, len(urls)):\n",
        "    download_url(urls[i], \"data\", class_name[i])\n",
        "    \n",
        "    \n",
        "  print(\"Done!\")   "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jPNS_XT5nCIN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **2.2 Reduction, reshape and reorganization of the Dataset:**"
      ]
    },
    {
      "metadata": {
        "id": "C7wbAlMHZ1kr",
        "colab_type": "code",
        "outputId": "bc8932f1-938f-42e5-8f4f-beef372deaa9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "cell_type": "code",
      "source": [
        "class_name = ['apple', 'banana', 'book', 'fork', 'key', 'ladder', 'pizza', 'stop_sign', 'tennis_racquet', 'wheel']\n",
        "step = ['train', 'validation', 'test']\n",
        "\n",
        "dire = r'data/'\n",
        "\n",
        "max_length = 100000 # Maximum number of files (drawings) per class\n",
        "percen=[0.6, 0.3, 0.1] # Percentage of training, validation and testing\n",
        "\n",
        "begin = [0, int(max_length * percen[0]), int(max_length * (percen[0] + percen[1])) + 1]\n",
        "end = [int(max_length * (percen[0])), int(max_length * (percen[0] + percen[1])) + 1, max_length]\n",
        "\n",
        "for c in range(0, len(class_name)):\n",
        "  print('Class ' + str(c+1) + ' out of ' + str(len(class_name)))\n",
        "  filename = dire + str(class_name[c]) + '.npy'\n",
        "  data = np.load(filename)\n",
        "  \n",
        "  for s in range(0, len(step)):\n",
        "    dire_step = str(dire) + str(step[s])\n",
        "    if not os.path.exists(dire_step):\n",
        "      os.makedirs(dire_step)\n",
        "    \n",
        "    for i in range(begin[s], end[s]):\n",
        "      dire_class = str(dire_step) + '/' + str(class_name[c])\n",
        "      if not os.path.exists(dire_class):\n",
        "        os.makedirs(dire_class)\n",
        "      \n",
        "      # Reshape the raw data into 28x28 images\n",
        "      data_sample = data[i,:].reshape((28, 28))\n",
        "      sample_name = class_name[c] + '_' + str(step[s]) + '_' + str(i)\n",
        "      np.save(os.path.join(dire_class, sample_name), data_sample)\n",
        "\n",
        "print('Done!')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Class 1 out of 10\n",
            "Class 2 out of 10\n",
            "Class 3 out of 10\n",
            "Class 4 out of 10\n",
            "Class 5 out of 10\n",
            "Class 6 out of 10\n",
            "Class 7 out of 10\n",
            "Class 8 out of 10\n",
            "Class 9 out of 10\n",
            "Class 10 out of 10\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HhoE0veRfHoT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **2.3 Dataset Visualization:**\n",
        "\n",
        "Visualization of a random image corresponding to the training set of images of the selected class. "
      ]
    },
    {
      "metadata": {
        "id": "jh0RVxYHnZbS",
        "colab_type": "code",
        "outputId": "b4d28617-2277-48fb-d4fc-bd814f023282",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        }
      },
      "cell_type": "code",
      "source": [
        "drawing_class = 8  # 0-apple, 1-banana, 2-book, 3-fork, 4-key, 5-ladder, 6-pizza, 7-stop_sign, 8-tennis_racquet, 9-wheel\n",
        "image_number=random.randint(1,max_length*percen[0])\n",
        "dire = r'data/train/' + str(class_name[drawing_class]) + '/' + str(class_name[drawing_class]) + '_' + 'train' + '_' + str(image_number) +'.npy'\n",
        "data = np.load(dire)\n",
        "plt.imshow(data)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFKCAYAAACU6307AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f8160c84fd0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "SM8HHsj3Xw9p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **3. Network Definition**"
      ]
    },
    {
      "metadata": {
        "id": "wbwgqtNKU8ZT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##**3.1 Mini-batch definition**"
      ]
    },
    {
      "metadata": {
        "id": "ZTTQPubmvVjj",
        "colab_type": "code",
        "outputId": "4d66dc9d-c70f-4912-f158-239491213d27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "def load_sample(x):\n",
        "\treturn np.load(x)\n",
        "\n",
        "\n",
        "bs = 32\n",
        "train_dir = r\"data/train\"\n",
        "val_dir = r\"data/validation\"\n",
        "test_dir = r\"data/test\"\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "train_dataset = datasets.DatasetFolder(train_dir, extensions = ['.npy'], loader = load_sample)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = bs, shuffle = True, num_workers = 2)\n",
        "train_iter = iter(train_loader)\n",
        "\n",
        "valid_dataset = datasets.DatasetFolder(train_dir, extensions = ['.npy'], loader = load_sample)\n",
        "valid_loader = torch.utils.data.DataLoader(train_dataset, batch_size = bs, shuffle = True, num_workers = 2)\n",
        "valid_iter = iter(train_loader)\n",
        "\n",
        "\n",
        "\n",
        "test_dataset = datasets.DatasetFolder(test_dir, extensions = ['.npy'], loader = load_sample)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = bs, shuffle = True, num_workers = 2)\n",
        "test_iter = iter(test_loader)\n",
        "\n",
        "\n",
        "\n",
        "batch, labels = train_iter.next()\n",
        "print(batch.size())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([32, 28, 28])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9Dbh7DIbgvgc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **3.2 CNN definition and Forward Pass**"
      ]
    },
    {
      "metadata": {
        "id": "6xNq_vmpwdTz",
        "colab_type": "code",
        "outputId": "b1d25834-f13e-4424-da4e-8f2877499654",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "  \n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, 3, padding = 1)\n",
        "        torch.nn.init.xavier_uniform_(self.conv1.weight)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 3, padding = 1)\n",
        "        torch.nn.init.xavier_uniform_(self.conv2.weight)\n",
        "        self.fc1 = nn.Linear(16 * 7 * 7, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(-1, 16 * 7 * 7)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "net = Net()\n",
        "net.to(device)\n",
        "print(net)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (fc1): Linear(in_features=784, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YAA_NJoVYejA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **3.3 Loss Function and Optimizer Definition**\n",
        "\n",
        "As it is a classification, we have chosen to used the Cross Entropy Loss. As the optimizer we will use the Gradient Descent algorithm, having the learning rate and momentum as hyperparameters."
      ]
    },
    {
      "metadata": {
        "id": "2_X0YAHg316V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.00001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "esOsTmQft5fZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **4. Network Training**"
      ]
    },
    {
      "metadata": {
        "id": "eKoLV6eSt85B",
        "colab_type": "code",
        "outputId": "e9ce9276-88a8-4601-8606-4ea35052eb72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 43785
        }
      },
      "cell_type": "code",
      "source": [
        "# To plot the results\n",
        "training_loss_list = []\n",
        "validation_loss_list = []\n",
        "\n",
        "\n",
        "for epoch in range(50):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        \n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        inputs = inputs.view(bs,1,28,28).float()\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = net(inputs)\n",
        "        outputs = outputs.to(device)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 200 == 199:    # print every 2000 mini-batches\n",
        "            print('[%d, %5d] Training loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 200))\n",
        "            training_loss_list.append(running_loss/200)\n",
        "            running_loss = 0.0\n",
        "            \n",
        "            with torch.no_grad():\n",
        "              valid_inputs, valid_labels = valid_iter.next()\n",
        "              valid_inputs = valid_inputs.view(bs, 1, 28, 28).float()\n",
        "              valid_inputs = valid_inputs.to(device)\n",
        "              valid_labels = valid_labels.to(device)\n",
        "              valid_outputs = net(valid_inputs)\n",
        "              valid_loss = criterion(valid_outputs, valid_labels)\n",
        "              print('[%d, %5d] Validation loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, valid_loss))\n",
        "              validation_loss_list.append(valid_loss)\n",
        "            \n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,   200] Training loss: 2.556\n",
            "[1,   200] Validation loss: 1.613\n",
            "[1,   400] Training loss: 1.556\n",
            "[1,   400] Validation loss: 1.101\n",
            "[1,   600] Training loss: 1.299\n",
            "[1,   600] Validation loss: 1.605\n",
            "[1,   800] Training loss: 1.152\n",
            "[1,   800] Validation loss: 1.151\n",
            "[1,  1000] Training loss: 1.062\n",
            "[1,  1000] Validation loss: 1.199\n",
            "[1,  1200] Training loss: 0.989\n",
            "[1,  1200] Validation loss: 1.528\n",
            "[1,  1400] Training loss: 0.933\n",
            "[1,  1400] Validation loss: 0.933\n",
            "[1,  1600] Training loss: 0.904\n",
            "[1,  1600] Validation loss: 1.194\n",
            "[1,  1800] Training loss: 0.856\n",
            "[1,  1800] Validation loss: 1.018\n",
            "[1,  2000] Training loss: 0.809\n",
            "[1,  2000] Validation loss: 0.849\n",
            "[1,  2200] Training loss: 0.793\n",
            "[1,  2200] Validation loss: 0.595\n",
            "[1,  2400] Training loss: 0.777\n",
            "[1,  2400] Validation loss: 0.834\n",
            "[1,  2600] Training loss: 0.734\n",
            "[1,  2600] Validation loss: 0.665\n",
            "[1,  2800] Training loss: 0.715\n",
            "[1,  2800] Validation loss: 0.942\n",
            "[1,  3000] Training loss: 0.693\n",
            "[1,  3000] Validation loss: 0.492\n",
            "[1,  3200] Training loss: 0.680\n",
            "[1,  3200] Validation loss: 0.600\n",
            "[1,  3400] Training loss: 0.666\n",
            "[1,  3400] Validation loss: 0.462\n",
            "[1,  3600] Training loss: 0.670\n",
            "[1,  3600] Validation loss: 0.590\n",
            "[1,  3800] Training loss: 0.652\n",
            "[1,  3800] Validation loss: 0.533\n",
            "[1,  4000] Training loss: 0.627\n",
            "[1,  4000] Validation loss: 0.835\n",
            "[1,  4200] Training loss: 0.642\n",
            "[1,  4200] Validation loss: 0.622\n",
            "[1,  4400] Training loss: 0.610\n",
            "[1,  4400] Validation loss: 0.595\n",
            "[1,  4600] Training loss: 0.601\n",
            "[1,  4600] Validation loss: 0.568\n",
            "[1,  4800] Training loss: 0.600\n",
            "[1,  4800] Validation loss: 0.622\n",
            "[1,  5000] Training loss: 0.612\n",
            "[1,  5000] Validation loss: 0.424\n",
            "[1,  5200] Training loss: 0.578\n",
            "[1,  5200] Validation loss: 0.610\n",
            "[1,  5400] Training loss: 0.601\n",
            "[1,  5400] Validation loss: 0.483\n",
            "[1,  5600] Training loss: 0.557\n",
            "[1,  5600] Validation loss: 0.456\n",
            "[1,  5800] Training loss: 0.564\n",
            "[1,  5800] Validation loss: 0.884\n",
            "[1,  6000] Training loss: 0.557\n",
            "[1,  6000] Validation loss: 0.485\n",
            "[1,  6200] Training loss: 0.567\n",
            "[1,  6200] Validation loss: 0.304\n",
            "[1,  6400] Training loss: 0.576\n",
            "[1,  6400] Validation loss: 0.823\n",
            "[1,  6600] Training loss: 0.525\n",
            "[1,  6600] Validation loss: 0.389\n",
            "[1,  6800] Training loss: 0.578\n",
            "[1,  6800] Validation loss: 0.578\n",
            "[1,  7000] Training loss: 0.551\n",
            "[1,  7000] Validation loss: 0.521\n",
            "[1,  7200] Training loss: 0.546\n",
            "[1,  7200] Validation loss: 0.534\n",
            "[1,  7400] Training loss: 0.536\n",
            "[1,  7400] Validation loss: 0.427\n",
            "[1,  7600] Training loss: 0.504\n",
            "[1,  7600] Validation loss: 0.829\n",
            "[1,  7800] Training loss: 0.513\n",
            "[1,  7800] Validation loss: 0.263\n",
            "[1,  8000] Training loss: 0.534\n",
            "[1,  8000] Validation loss: 0.438\n",
            "[1,  8200] Training loss: 0.522\n",
            "[1,  8200] Validation loss: 0.505\n",
            "[1,  8400] Training loss: 0.504\n",
            "[1,  8400] Validation loss: 0.258\n",
            "[1,  8600] Training loss: 0.525\n",
            "[1,  8600] Validation loss: 0.429\n",
            "[1,  8800] Training loss: 0.552\n",
            "[1,  8800] Validation loss: 0.377\n",
            "[1,  9000] Training loss: 0.511\n",
            "[1,  9000] Validation loss: 0.435\n",
            "[1,  9200] Training loss: 0.526\n",
            "[1,  9200] Validation loss: 0.731\n",
            "[1,  9400] Training loss: 0.504\n",
            "[1,  9400] Validation loss: 0.727\n",
            "[1,  9600] Training loss: 0.500\n",
            "[1,  9600] Validation loss: 0.615\n",
            "[1,  9800] Training loss: 0.485\n",
            "[1,  9800] Validation loss: 0.427\n",
            "[1, 10000] Training loss: 0.508\n",
            "[1, 10000] Validation loss: 0.480\n",
            "[1, 10200] Training loss: 0.479\n",
            "[1, 10200] Validation loss: 0.551\n",
            "[1, 10400] Training loss: 0.490\n",
            "[1, 10400] Validation loss: 0.497\n",
            "[1, 10600] Training loss: 0.525\n",
            "[1, 10600] Validation loss: 0.605\n",
            "[1, 10800] Training loss: 0.485\n",
            "[1, 10800] Validation loss: 0.534\n",
            "[1, 11000] Training loss: 0.490\n",
            "[1, 11000] Validation loss: 0.364\n",
            "[1, 11200] Training loss: 0.486\n",
            "[1, 11200] Validation loss: 0.498\n",
            "[1, 11400] Training loss: 0.477\n",
            "[1, 11400] Validation loss: 0.785\n",
            "[1, 11600] Training loss: 0.485\n",
            "[1, 11600] Validation loss: 0.837\n",
            "[1, 11800] Training loss: 0.469\n",
            "[1, 11800] Validation loss: 0.499\n",
            "[1, 12000] Training loss: 0.472\n",
            "[1, 12000] Validation loss: 0.528\n",
            "[1, 12200] Training loss: 0.464\n",
            "[1, 12200] Validation loss: 0.410\n",
            "[1, 12400] Training loss: 0.457\n",
            "[1, 12400] Validation loss: 0.183\n",
            "[1, 12600] Training loss: 0.471\n",
            "[1, 12600] Validation loss: 0.283\n",
            "[1, 12800] Training loss: 0.453\n",
            "[1, 12800] Validation loss: 0.297\n",
            "[1, 13000] Training loss: 0.451\n",
            "[1, 13000] Validation loss: 0.486\n",
            "[1, 13200] Training loss: 0.463\n",
            "[1, 13200] Validation loss: 0.627\n",
            "[1, 13400] Training loss: 0.465\n",
            "[1, 13400] Validation loss: 0.303\n",
            "[1, 13600] Training loss: 0.456\n",
            "[1, 13600] Validation loss: 0.661\n",
            "[1, 13800] Training loss: 0.444\n",
            "[1, 13800] Validation loss: 0.551\n",
            "[1, 14000] Training loss: 0.459\n",
            "[1, 14000] Validation loss: 0.326\n",
            "[1, 14200] Training loss: 0.476\n",
            "[1, 14200] Validation loss: 0.763\n",
            "[1, 14400] Training loss: 0.458\n",
            "[1, 14400] Validation loss: 0.467\n",
            "[1, 14600] Training loss: 0.452\n",
            "[1, 14600] Validation loss: 0.407\n",
            "[1, 14800] Training loss: 0.439\n",
            "[1, 14800] Validation loss: 0.391\n",
            "[1, 15000] Training loss: 0.443\n",
            "[1, 15000] Validation loss: 0.814\n",
            "[1, 15200] Training loss: 0.445\n",
            "[1, 15200] Validation loss: 0.285\n",
            "[1, 15400] Training loss: 0.459\n",
            "[1, 15400] Validation loss: 0.449\n",
            "[1, 15600] Training loss: 0.448\n",
            "[1, 15600] Validation loss: 0.504\n",
            "[1, 15800] Training loss: 0.439\n",
            "[1, 15800] Validation loss: 0.824\n",
            "[1, 16000] Training loss: 0.445\n",
            "[1, 16000] Validation loss: 0.219\n",
            "[1, 16200] Training loss: 0.430\n",
            "[1, 16200] Validation loss: 0.318\n",
            "[1, 16400] Training loss: 0.420\n",
            "[1, 16400] Validation loss: 0.503\n",
            "[1, 16600] Training loss: 0.447\n",
            "[1, 16600] Validation loss: 0.262\n",
            "[1, 16800] Training loss: 0.452\n",
            "[1, 16800] Validation loss: 0.771\n",
            "[1, 17000] Training loss: 0.447\n",
            "[1, 17000] Validation loss: 0.306\n",
            "[1, 17200] Training loss: 0.418\n",
            "[1, 17200] Validation loss: 0.724\n",
            "[1, 17400] Training loss: 0.428\n",
            "[1, 17400] Validation loss: 0.547\n",
            "[1, 17600] Training loss: 0.445\n",
            "[1, 17600] Validation loss: 0.564\n",
            "[1, 17800] Training loss: 0.438\n",
            "[1, 17800] Validation loss: 0.376\n",
            "[1, 18000] Training loss: 0.428\n",
            "[1, 18000] Validation loss: 0.233\n",
            "[1, 18200] Training loss: 0.433\n",
            "[1, 18200] Validation loss: 0.259\n",
            "[1, 18400] Training loss: 0.435\n",
            "[1, 18400] Validation loss: 0.391\n",
            "[1, 18600] Training loss: 0.434\n",
            "[1, 18600] Validation loss: 0.447\n",
            "[2,   200] Training loss: 0.407\n",
            "[2,   200] Validation loss: 0.525\n",
            "[2,   400] Training loss: 0.429\n",
            "[2,   400] Validation loss: 0.585\n",
            "[2,   600] Training loss: 0.406\n",
            "[2,   600] Validation loss: 0.352\n",
            "[2,   800] Training loss: 0.408\n",
            "[2,   800] Validation loss: 0.231\n",
            "[2,  1000] Training loss: 0.415\n",
            "[2,  1000] Validation loss: 0.286\n",
            "[2,  1200] Training loss: 0.393\n",
            "[2,  1200] Validation loss: 0.376\n",
            "[2,  1400] Training loss: 0.397\n",
            "[2,  1400] Validation loss: 0.466\n",
            "[2,  1600] Training loss: 0.409\n",
            "[2,  1600] Validation loss: 0.380\n",
            "[2,  1800] Training loss: 0.414\n",
            "[2,  1800] Validation loss: 0.250\n",
            "[2,  2000] Training loss: 0.418\n",
            "[2,  2000] Validation loss: 0.382\n",
            "[2,  2200] Training loss: 0.418\n",
            "[2,  2200] Validation loss: 0.324\n",
            "[2,  2400] Training loss: 0.417\n",
            "[2,  2400] Validation loss: 0.398\n",
            "[2,  2600] Training loss: 0.412\n",
            "[2,  2600] Validation loss: 0.343\n",
            "[2,  2800] Training loss: 0.408\n",
            "[2,  2800] Validation loss: 0.278\n",
            "[2,  3000] Training loss: 0.418\n",
            "[2,  3000] Validation loss: 0.324\n",
            "[2,  3200] Training loss: 0.386\n",
            "[2,  3200] Validation loss: 0.307\n",
            "[2,  3400] Training loss: 0.413\n",
            "[2,  3400] Validation loss: 0.253\n",
            "[2,  3600] Training loss: 0.415\n",
            "[2,  3600] Validation loss: 0.467\n",
            "[2,  3800] Training loss: 0.392\n",
            "[2,  3800] Validation loss: 0.180\n",
            "[2,  4000] Training loss: 0.431\n",
            "[2,  4000] Validation loss: 0.370\n",
            "[2,  4200] Training loss: 0.433\n",
            "[2,  4200] Validation loss: 0.439\n",
            "[2,  4400] Training loss: 0.413\n",
            "[2,  4400] Validation loss: 0.646\n",
            "[2,  4600] Training loss: 0.411\n",
            "[2,  4600] Validation loss: 0.223\n",
            "[2,  4800] Training loss: 0.393\n",
            "[2,  4800] Validation loss: 0.384\n",
            "[2,  5000] Training loss: 0.388\n",
            "[2,  5000] Validation loss: 0.241\n",
            "[2,  5200] Training loss: 0.410\n",
            "[2,  5200] Validation loss: 0.195\n",
            "[2,  5400] Training loss: 0.394\n",
            "[2,  5400] Validation loss: 0.137\n",
            "[2,  5600] Training loss: 0.392\n",
            "[2,  5600] Validation loss: 0.492\n",
            "[2,  5800] Training loss: 0.398\n",
            "[2,  5800] Validation loss: 0.350\n",
            "[2,  6000] Training loss: 0.386\n",
            "[2,  6000] Validation loss: 0.447\n",
            "[2,  6200] Training loss: 0.402\n",
            "[2,  6200] Validation loss: 0.314\n",
            "[2,  6400] Training loss: 0.398\n",
            "[2,  6400] Validation loss: 0.793\n",
            "[2,  6600] Training loss: 0.380\n",
            "[2,  6600] Validation loss: 0.329\n",
            "[2,  6800] Training loss: 0.389\n",
            "[2,  6800] Validation loss: 0.499\n",
            "[2,  7000] Training loss: 0.402\n",
            "[2,  7000] Validation loss: 0.210\n",
            "[2,  7200] Training loss: 0.384\n",
            "[2,  7200] Validation loss: 0.207\n",
            "[2,  7400] Training loss: 0.384\n",
            "[2,  7400] Validation loss: 0.493\n",
            "[2,  7600] Training loss: 0.395\n",
            "[2,  7600] Validation loss: 0.635\n",
            "[2,  7800] Training loss: 0.399\n",
            "[2,  7800] Validation loss: 0.243\n",
            "[2,  8000] Training loss: 0.395\n",
            "[2,  8000] Validation loss: 0.604\n",
            "[2,  8200] Training loss: 0.387\n",
            "[2,  8200] Validation loss: 0.257\n",
            "[2,  8400] Training loss: 0.401\n",
            "[2,  8400] Validation loss: 0.353\n",
            "[2,  8600] Training loss: 0.387\n",
            "[2,  8600] Validation loss: 0.286\n",
            "[2,  8800] Training loss: 0.376\n",
            "[2,  8800] Validation loss: 0.491\n",
            "[2,  9000] Training loss: 0.412\n",
            "[2,  9000] Validation loss: 0.311\n",
            "[2,  9200] Training loss: 0.404\n",
            "[2,  9200] Validation loss: 0.458\n",
            "[2,  9400] Training loss: 0.396\n",
            "[2,  9400] Validation loss: 0.455\n",
            "[2,  9600] Training loss: 0.392\n",
            "[2,  9600] Validation loss: 0.222\n",
            "[2,  9800] Training loss: 0.380\n",
            "[2,  9800] Validation loss: 0.835\n",
            "[2, 10000] Training loss: 0.379\n",
            "[2, 10000] Validation loss: 0.498\n",
            "[2, 10200] Training loss: 0.388\n",
            "[2, 10200] Validation loss: 0.417\n",
            "[2, 10400] Training loss: 0.390\n",
            "[2, 10400] Validation loss: 0.318\n",
            "[2, 10600] Training loss: 0.378\n",
            "[2, 10600] Validation loss: 0.621\n",
            "[2, 10800] Training loss: 0.395\n",
            "[2, 10800] Validation loss: 0.305\n",
            "[2, 11000] Training loss: 0.375\n",
            "[2, 11000] Validation loss: 0.304\n",
            "[2, 11200] Training loss: 0.385\n",
            "[2, 11200] Validation loss: 0.285\n",
            "[2, 11400] Training loss: 0.388\n",
            "[2, 11400] Validation loss: 0.502\n",
            "[2, 11600] Training loss: 0.411\n",
            "[2, 11600] Validation loss: 0.473\n",
            "[2, 11800] Training loss: 0.373\n",
            "[2, 11800] Validation loss: 0.509\n",
            "[2, 12000] Training loss: 0.376\n",
            "[2, 12000] Validation loss: 0.225\n",
            "[2, 12200] Training loss: 0.381\n",
            "[2, 12200] Validation loss: 0.293\n",
            "[2, 12400] Training loss: 0.384\n",
            "[2, 12400] Validation loss: 0.252\n",
            "[2, 12600] Training loss: 0.375\n",
            "[2, 12600] Validation loss: 0.201\n",
            "[2, 12800] Training loss: 0.382\n",
            "[2, 12800] Validation loss: 0.458\n",
            "[2, 13000] Training loss: 0.377\n",
            "[2, 13000] Validation loss: 0.400\n",
            "[2, 13200] Training loss: 0.372\n",
            "[2, 13200] Validation loss: 0.254\n",
            "[2, 13400] Training loss: 0.363\n",
            "[2, 13400] Validation loss: 0.247\n",
            "[2, 13600] Training loss: 0.387\n",
            "[2, 13600] Validation loss: 0.371\n",
            "[2, 13800] Training loss: 0.382\n",
            "[2, 13800] Validation loss: 0.169\n",
            "[2, 14000] Training loss: 0.367\n",
            "[2, 14000] Validation loss: 0.330\n",
            "[2, 14200] Training loss: 0.380\n",
            "[2, 14200] Validation loss: 0.297\n",
            "[2, 14400] Training loss: 0.360\n",
            "[2, 14400] Validation loss: 0.268\n",
            "[2, 14600] Training loss: 0.364\n",
            "[2, 14600] Validation loss: 0.432\n",
            "[2, 14800] Training loss: 0.363\n",
            "[2, 14800] Validation loss: 0.169\n",
            "[2, 15000] Training loss: 0.396\n",
            "[2, 15000] Validation loss: 0.361\n",
            "[2, 15200] Training loss: 0.371\n",
            "[2, 15200] Validation loss: 0.163\n",
            "[2, 15400] Training loss: 0.392\n",
            "[2, 15400] Validation loss: 0.433\n",
            "[2, 15600] Training loss: 0.401\n",
            "[2, 15600] Validation loss: 0.309\n",
            "[2, 15800] Training loss: 0.378\n",
            "[2, 15800] Validation loss: 0.512\n",
            "[2, 16000] Training loss: 0.355\n",
            "[2, 16000] Validation loss: 0.419\n",
            "[2, 16200] Training loss: 0.377\n",
            "[2, 16200] Validation loss: 0.331\n",
            "[2, 16400] Training loss: 0.382\n",
            "[2, 16400] Validation loss: 0.561\n",
            "[2, 16600] Training loss: 0.366\n",
            "[2, 16600] Validation loss: 0.557\n",
            "[2, 16800] Training loss: 0.370\n",
            "[2, 16800] Validation loss: 0.094\n",
            "[2, 17000] Training loss: 0.359\n",
            "[2, 17000] Validation loss: 0.392\n",
            "[2, 17200] Training loss: 0.357\n",
            "[2, 17200] Validation loss: 0.125\n",
            "[2, 17400] Training loss: 0.400\n",
            "[2, 17400] Validation loss: 0.209\n",
            "[2, 17600] Training loss: 0.392\n",
            "[2, 17600] Validation loss: 0.591\n",
            "[2, 17800] Training loss: 0.356\n",
            "[2, 17800] Validation loss: 0.334\n",
            "[2, 18000] Training loss: 0.371\n",
            "[2, 18000] Validation loss: 0.203\n",
            "[2, 18200] Training loss: 0.367\n",
            "[2, 18200] Validation loss: 0.455\n",
            "[2, 18400] Training loss: 0.369\n",
            "[2, 18400] Validation loss: 0.479\n",
            "[2, 18600] Training loss: 0.374\n",
            "[2, 18600] Validation loss: 0.228\n",
            "[3,   200] Training loss: 0.345\n",
            "[3,   200] Validation loss: 0.526\n",
            "[3,   400] Training loss: 0.377\n",
            "[3,   400] Validation loss: 0.326\n",
            "[3,   600] Training loss: 0.348\n",
            "[3,   600] Validation loss: 0.190\n",
            "[3,   800] Training loss: 0.378\n",
            "[3,   800] Validation loss: 0.313\n",
            "[3,  1000] Training loss: 0.357\n",
            "[3,  1000] Validation loss: 0.395\n",
            "[3,  1200] Training loss: 0.360\n",
            "[3,  1200] Validation loss: 0.278\n",
            "[3,  1400] Training loss: 0.357\n",
            "[3,  1400] Validation loss: 0.437\n",
            "[3,  1600] Training loss: 0.354\n",
            "[3,  1600] Validation loss: 0.736\n",
            "[3,  1800] Training loss: 0.341\n",
            "[3,  1800] Validation loss: 0.314\n",
            "[3,  2000] Training loss: 0.374\n",
            "[3,  2000] Validation loss: 0.186\n",
            "[3,  2200] Training loss: 0.388\n",
            "[3,  2200] Validation loss: 0.325\n",
            "[3,  2400] Training loss: 0.352\n",
            "[3,  2400] Validation loss: 0.295\n",
            "[3,  2600] Training loss: 0.363\n",
            "[3,  2600] Validation loss: 0.319\n",
            "[3,  2800] Training loss: 0.355\n",
            "[3,  2800] Validation loss: 0.392\n",
            "[3,  3000] Training loss: 0.370\n",
            "[3,  3000] Validation loss: 0.402\n",
            "[3,  3200] Training loss: 0.352\n",
            "[3,  3200] Validation loss: 0.541\n",
            "[3,  3400] Training loss: 0.355\n",
            "[3,  3400] Validation loss: 0.415\n",
            "[3,  3600] Training loss: 0.351\n",
            "[3,  3600] Validation loss: 0.240\n",
            "[3,  3800] Training loss: 0.358\n",
            "[3,  3800] Validation loss: 0.250\n",
            "[3,  4000] Training loss: 0.352\n",
            "[3,  4000] Validation loss: 0.826\n",
            "[3,  4200] Training loss: 0.376\n",
            "[3,  4200] Validation loss: 0.609\n",
            "[3,  4400] Training loss: 0.352\n",
            "[3,  4400] Validation loss: 0.197\n",
            "[3,  4600] Training loss: 0.345\n",
            "[3,  4600] Validation loss: 0.296\n",
            "[3,  4800] Training loss: 0.365\n",
            "[3,  4800] Validation loss: 0.313\n",
            "[3,  5000] Training loss: 0.344\n",
            "[3,  5000] Validation loss: 0.611\n",
            "[3,  5200] Training loss: 0.379\n",
            "[3,  5200] Validation loss: 0.405\n",
            "[3,  5400] Training loss: 0.348\n",
            "[3,  5400] Validation loss: 0.229\n",
            "[3,  5600] Training loss: 0.346\n",
            "[3,  5600] Validation loss: 0.191\n",
            "[3,  5800] Training loss: 0.330\n",
            "[3,  5800] Validation loss: 0.190\n",
            "[3,  6000] Training loss: 0.350\n",
            "[3,  6000] Validation loss: 0.133\n",
            "[3,  6200] Training loss: 0.347\n",
            "[3,  6200] Validation loss: 0.227\n",
            "[3,  6400] Training loss: 0.346\n",
            "[3,  6400] Validation loss: 0.417\n",
            "[3,  6600] Training loss: 0.351\n",
            "[3,  6600] Validation loss: 0.278\n",
            "[3,  6800] Training loss: 0.371\n",
            "[3,  6800] Validation loss: 0.320\n",
            "[3,  7000] Training loss: 0.354\n",
            "[3,  7000] Validation loss: 0.312\n",
            "[3,  7200] Training loss: 0.355\n",
            "[3,  7200] Validation loss: 0.180\n",
            "[3,  7400] Training loss: 0.343\n",
            "[3,  7400] Validation loss: 0.535\n",
            "[3,  7600] Training loss: 0.348\n",
            "[3,  7600] Validation loss: 0.275\n",
            "[3,  7800] Training loss: 0.337\n",
            "[3,  7800] Validation loss: 0.503\n",
            "[3,  8000] Training loss: 0.308\n",
            "[3,  8000] Validation loss: 0.342\n",
            "[3,  8200] Training loss: 0.347\n",
            "[3,  8200] Validation loss: 0.300\n",
            "[3,  8400] Training loss: 0.356\n",
            "[3,  8400] Validation loss: 0.298\n",
            "[3,  8600] Training loss: 0.342\n",
            "[3,  8600] Validation loss: 0.318\n",
            "[3,  8800] Training loss: 0.330\n",
            "[3,  8800] Validation loss: 0.359\n",
            "[3,  9000] Training loss: 0.345\n",
            "[3,  9000] Validation loss: 0.209\n",
            "[3,  9200] Training loss: 0.345\n",
            "[3,  9200] Validation loss: 0.486\n",
            "[3,  9400] Training loss: 0.344\n",
            "[3,  9400] Validation loss: 0.189\n",
            "[3,  9600] Training loss: 0.350\n",
            "[3,  9600] Validation loss: 0.352\n",
            "[3,  9800] Training loss: 0.328\n",
            "[3,  9800] Validation loss: 0.414\n",
            "[3, 10000] Training loss: 0.346\n",
            "[3, 10000] Validation loss: 0.172\n",
            "[3, 10200] Training loss: 0.350\n",
            "[3, 10200] Validation loss: 0.355\n",
            "[3, 10400] Training loss: 0.340\n",
            "[3, 10400] Validation loss: 0.237\n",
            "[3, 10600] Training loss: 0.342\n",
            "[3, 10600] Validation loss: 0.710\n",
            "[3, 10800] Training loss: 0.352\n",
            "[3, 10800] Validation loss: 0.218\n",
            "[3, 11000] Training loss: 0.356\n",
            "[3, 11000] Validation loss: 0.366\n",
            "[3, 11200] Training loss: 0.342\n",
            "[3, 11200] Validation loss: 0.335\n",
            "[3, 11400] Training loss: 0.338\n",
            "[3, 11400] Validation loss: 0.252\n",
            "[3, 11600] Training loss: 0.343\n",
            "[3, 11600] Validation loss: 0.318\n",
            "[3, 11800] Training loss: 0.338\n",
            "[3, 11800] Validation loss: 0.232\n",
            "[3, 12000] Training loss: 0.341\n",
            "[3, 12000] Validation loss: 0.196\n",
            "[3, 12200] Training loss: 0.344\n",
            "[3, 12200] Validation loss: 0.237\n",
            "[3, 12400] Training loss: 0.349\n",
            "[3, 12400] Validation loss: 0.195\n",
            "[3, 12600] Training loss: 0.353\n",
            "[3, 12600] Validation loss: 0.346\n",
            "[3, 12800] Training loss: 0.358\n",
            "[3, 12800] Validation loss: 0.110\n",
            "[3, 13000] Training loss: 0.351\n",
            "[3, 13000] Validation loss: 0.216\n",
            "[3, 13200] Training loss: 0.334\n",
            "[3, 13200] Validation loss: 0.478\n",
            "[3, 13400] Training loss: 0.339\n",
            "[3, 13400] Validation loss: 0.192\n",
            "[3, 13600] Training loss: 0.360\n",
            "[3, 13600] Validation loss: 0.495\n",
            "[3, 13800] Training loss: 0.333\n",
            "[3, 13800] Validation loss: 0.298\n",
            "[3, 14000] Training loss: 0.352\n",
            "[3, 14000] Validation loss: 0.268\n",
            "[3, 14200] Training loss: 0.345\n",
            "[3, 14200] Validation loss: 0.218\n",
            "[3, 14400] Training loss: 0.339\n",
            "[3, 14400] Validation loss: 0.322\n",
            "[3, 14600] Training loss: 0.331\n",
            "[3, 14600] Validation loss: 0.221\n",
            "[3, 14800] Training loss: 0.330\n",
            "[3, 14800] Validation loss: 0.183\n",
            "[3, 15000] Training loss: 0.320\n",
            "[3, 15000] Validation loss: 0.491\n",
            "[3, 15200] Training loss: 0.317\n",
            "[3, 15200] Validation loss: 0.237\n",
            "[3, 15400] Training loss: 0.340\n",
            "[3, 15400] Validation loss: 0.405\n",
            "[3, 15600] Training loss: 0.355\n",
            "[3, 15600] Validation loss: 0.183\n",
            "[3, 15800] Training loss: 0.350\n",
            "[3, 15800] Validation loss: 0.409\n",
            "[3, 16000] Training loss: 0.355\n",
            "[3, 16000] Validation loss: 0.735\n",
            "[3, 16200] Training loss: 0.345\n",
            "[3, 16200] Validation loss: 0.260\n",
            "[3, 16400] Training loss: 0.344\n",
            "[3, 16400] Validation loss: 0.575\n",
            "[3, 16600] Training loss: 0.350\n",
            "[3, 16600] Validation loss: 0.497\n",
            "[3, 16800] Training loss: 0.323\n",
            "[3, 16800] Validation loss: 0.521\n",
            "[3, 17000] Training loss: 0.323\n",
            "[3, 17000] Validation loss: 0.097\n",
            "[3, 17200] Training loss: 0.340\n",
            "[3, 17200] Validation loss: 0.465\n",
            "[3, 17400] Training loss: 0.348\n",
            "[3, 17400] Validation loss: 0.400\n",
            "[3, 17600] Training loss: 0.334\n",
            "[3, 17600] Validation loss: 0.179\n",
            "[3, 17800] Training loss: 0.350\n",
            "[3, 17800] Validation loss: 0.140\n",
            "[3, 18000] Training loss: 0.336\n",
            "[3, 18000] Validation loss: 0.497\n",
            "[3, 18200] Training loss: 0.339\n",
            "[3, 18200] Validation loss: 0.217\n",
            "[3, 18400] Training loss: 0.330\n",
            "[3, 18400] Validation loss: 0.160\n",
            "[3, 18600] Training loss: 0.319\n",
            "[3, 18600] Validation loss: 0.373\n",
            "[4,   200] Training loss: 0.338\n",
            "[4,   200] Validation loss: 0.228\n",
            "[4,   400] Training loss: 0.326\n",
            "[4,   400] Validation loss: 0.377\n",
            "[4,   600] Training loss: 0.347\n",
            "[4,   600] Validation loss: 0.345\n",
            "[4,   800] Training loss: 0.329\n",
            "[4,   800] Validation loss: 0.061\n",
            "[4,  1000] Training loss: 0.334\n",
            "[4,  1000] Validation loss: 0.141\n",
            "[4,  1200] Training loss: 0.335\n",
            "[4,  1200] Validation loss: 0.214\n",
            "[4,  1400] Training loss: 0.337\n",
            "[4,  1400] Validation loss: 0.294\n",
            "[4,  1600] Training loss: 0.331\n",
            "[4,  1600] Validation loss: 0.394\n",
            "[4,  1800] Training loss: 0.329\n",
            "[4,  1800] Validation loss: 0.329\n",
            "[4,  2000] Training loss: 0.320\n",
            "[4,  2000] Validation loss: 0.070\n",
            "[4,  2200] Training loss: 0.333\n",
            "[4,  2200] Validation loss: 0.261\n",
            "[4,  2400] Training loss: 0.311\n",
            "[4,  2400] Validation loss: 0.263\n",
            "[4,  2600] Training loss: 0.326\n",
            "[4,  2600] Validation loss: 0.205\n",
            "[4,  2800] Training loss: 0.318\n",
            "[4,  2800] Validation loss: 0.479\n",
            "[4,  3000] Training loss: 0.329\n",
            "[4,  3000] Validation loss: 0.398\n",
            "[4,  3200] Training loss: 0.320\n",
            "[4,  3200] Validation loss: 0.515\n",
            "[4,  3400] Training loss: 0.329\n",
            "[4,  3400] Validation loss: 0.361\n",
            "[4,  3600] Training loss: 0.341\n",
            "[4,  3600] Validation loss: 0.247\n",
            "[4,  3800] Training loss: 0.334\n",
            "[4,  3800] Validation loss: 0.344\n",
            "[4,  4000] Training loss: 0.325\n",
            "[4,  4000] Validation loss: 0.381\n",
            "[4,  4200] Training loss: 0.323\n",
            "[4,  4200] Validation loss: 0.322\n",
            "[4,  4400] Training loss: 0.322\n",
            "[4,  4400] Validation loss: 0.360\n",
            "[4,  4600] Training loss: 0.325\n",
            "[4,  4600] Validation loss: 0.231\n",
            "[4,  4800] Training loss: 0.332\n",
            "[4,  4800] Validation loss: 0.092\n",
            "[4,  5000] Training loss: 0.302\n",
            "[4,  5000] Validation loss: 0.256\n",
            "[4,  5200] Training loss: 0.319\n",
            "[4,  5200] Validation loss: 0.153\n",
            "[4,  5400] Training loss: 0.322\n",
            "[4,  5400] Validation loss: 0.412\n",
            "[4,  5600] Training loss: 0.344\n",
            "[4,  5600] Validation loss: 0.418\n",
            "[4,  5800] Training loss: 0.333\n",
            "[4,  5800] Validation loss: 0.371\n",
            "[4,  6000] Training loss: 0.302\n",
            "[4,  6000] Validation loss: 0.237\n",
            "[4,  6200] Training loss: 0.328\n",
            "[4,  6200] Validation loss: 0.455\n",
            "[4,  6400] Training loss: 0.311\n",
            "[4,  6400] Validation loss: 0.788\n",
            "[4,  6600] Training loss: 0.323\n",
            "[4,  6600] Validation loss: 0.728\n",
            "[4,  6800] Training loss: 0.322\n",
            "[4,  6800] Validation loss: 0.339\n",
            "[4,  7000] Training loss: 0.327\n",
            "[4,  7000] Validation loss: 0.524\n",
            "[4,  7200] Training loss: 0.338\n",
            "[4,  7200] Validation loss: 0.264\n",
            "[4,  7400] Training loss: 0.307\n",
            "[4,  7400] Validation loss: 0.247\n",
            "[4,  7600] Training loss: 0.321\n",
            "[4,  7600] Validation loss: 0.533\n",
            "[4,  7800] Training loss: 0.319\n",
            "[4,  7800] Validation loss: 0.131\n",
            "[4,  8000] Training loss: 0.336\n",
            "[4,  8000] Validation loss: 0.253\n",
            "[4,  8200] Training loss: 0.316\n",
            "[4,  8200] Validation loss: 0.271\n",
            "[4,  8400] Training loss: 0.335\n",
            "[4,  8400] Validation loss: 0.324\n",
            "[4,  8600] Training loss: 0.321\n",
            "[4,  8600] Validation loss: 0.274\n",
            "[4,  8800] Training loss: 0.315\n",
            "[4,  8800] Validation loss: 0.365\n",
            "[4,  9000] Training loss: 0.332\n",
            "[4,  9000] Validation loss: 0.281\n",
            "[4,  9200] Training loss: 0.331\n",
            "[4,  9200] Validation loss: 0.295\n",
            "[4,  9400] Training loss: 0.330\n",
            "[4,  9400] Validation loss: 0.397\n",
            "[4,  9600] Training loss: 0.327\n",
            "[4,  9600] Validation loss: 0.217\n",
            "[4,  9800] Training loss: 0.325\n",
            "[4,  9800] Validation loss: 0.313\n",
            "[4, 10000] Training loss: 0.325\n",
            "[4, 10000] Validation loss: 0.273\n",
            "[4, 10200] Training loss: 0.335\n",
            "[4, 10200] Validation loss: 0.471\n",
            "[4, 10400] Training loss: 0.323\n",
            "[4, 10400] Validation loss: 0.319\n",
            "[4, 10600] Training loss: 0.322\n",
            "[4, 10600] Validation loss: 0.513\n",
            "[4, 10800] Training loss: 0.330\n",
            "[4, 10800] Validation loss: 0.159\n",
            "[4, 11000] Training loss: 0.315\n",
            "[4, 11000] Validation loss: 0.407\n",
            "[4, 11200] Training loss: 0.323\n",
            "[4, 11200] Validation loss: 0.242\n",
            "[4, 11400] Training loss: 0.326\n",
            "[4, 11400] Validation loss: 0.228\n",
            "[4, 11600] Training loss: 0.326\n",
            "[4, 11600] Validation loss: 0.427\n",
            "[4, 11800] Training loss: 0.320\n",
            "[4, 11800] Validation loss: 0.246\n",
            "[4, 12000] Training loss: 0.313\n",
            "[4, 12000] Validation loss: 0.380\n",
            "[4, 12200] Training loss: 0.314\n",
            "[4, 12200] Validation loss: 0.316\n",
            "[4, 12400] Training loss: 0.317\n",
            "[4, 12400] Validation loss: 0.338\n",
            "[4, 12600] Training loss: 0.314\n",
            "[4, 12600] Validation loss: 0.063\n",
            "[4, 12800] Training loss: 0.328\n",
            "[4, 12800] Validation loss: 0.287\n",
            "[4, 13000] Training loss: 0.333\n",
            "[4, 13000] Validation loss: 0.454\n",
            "[4, 13200] Training loss: 0.312\n",
            "[4, 13200] Validation loss: 0.503\n",
            "[4, 13400] Training loss: 0.321\n",
            "[4, 13400] Validation loss: 0.446\n",
            "[4, 13600] Training loss: 0.313\n",
            "[4, 13600] Validation loss: 0.189\n",
            "[4, 13800] Training loss: 0.316\n",
            "[4, 13800] Validation loss: 0.619\n",
            "[4, 14000] Training loss: 0.324\n",
            "[4, 14000] Validation loss: 0.152\n",
            "[4, 14200] Training loss: 0.303\n",
            "[4, 14200] Validation loss: 0.518\n",
            "[4, 14400] Training loss: 0.336\n",
            "[4, 14400] Validation loss: 0.179\n",
            "[4, 14600] Training loss: 0.334\n",
            "[4, 14600] Validation loss: 0.444\n",
            "[4, 14800] Training loss: 0.327\n",
            "[4, 14800] Validation loss: 0.321\n",
            "[4, 15000] Training loss: 0.297\n",
            "[4, 15000] Validation loss: 0.156\n",
            "[4, 15200] Training loss: 0.331\n",
            "[4, 15200] Validation loss: 0.498\n",
            "[4, 15400] Training loss: 0.311\n",
            "[4, 15400] Validation loss: 0.534\n",
            "[4, 15600] Training loss: 0.310\n",
            "[4, 15600] Validation loss: 0.598\n",
            "[4, 15800] Training loss: 0.313\n",
            "[4, 15800] Validation loss: 0.280\n",
            "[4, 16000] Training loss: 0.313\n",
            "[4, 16000] Validation loss: 0.226\n",
            "[4, 16200] Training loss: 0.316\n",
            "[4, 16200] Validation loss: 0.127\n",
            "[4, 16400] Training loss: 0.326\n",
            "[4, 16400] Validation loss: 0.753\n",
            "[4, 16600] Training loss: 0.292\n",
            "[4, 16600] Validation loss: 0.264\n",
            "[4, 16800] Training loss: 0.324\n",
            "[4, 16800] Validation loss: 0.436\n",
            "[4, 17000] Training loss: 0.315\n",
            "[4, 17000] Validation loss: 0.198\n",
            "[4, 17200] Training loss: 0.321\n",
            "[4, 17200] Validation loss: 0.587\n",
            "[4, 17400] Training loss: 0.332\n",
            "[4, 17400] Validation loss: 0.081\n",
            "[4, 17600] Training loss: 0.325\n",
            "[4, 17600] Validation loss: 0.272\n",
            "[4, 17800] Training loss: 0.312\n",
            "[4, 17800] Validation loss: 0.181\n",
            "[4, 18000] Training loss: 0.316\n",
            "[4, 18000] Validation loss: 0.196\n",
            "[4, 18200] Training loss: 0.311\n",
            "[4, 18200] Validation loss: 0.380\n",
            "[4, 18400] Training loss: 0.319\n",
            "[4, 18400] Validation loss: 0.285\n",
            "[4, 18600] Training loss: 0.330\n",
            "[4, 18600] Validation loss: 0.286\n",
            "[5,   200] Training loss: 0.303\n",
            "[5,   200] Validation loss: 0.294\n",
            "[5,   400] Training loss: 0.312\n",
            "[5,   400] Validation loss: 0.417\n",
            "[5,   600] Training loss: 0.290\n",
            "[5,   600] Validation loss: 0.289\n",
            "[5,   800] Training loss: 0.302\n",
            "[5,   800] Validation loss: 0.157\n",
            "[5,  1000] Training loss: 0.298\n",
            "[5,  1000] Validation loss: 0.296\n",
            "[5,  1200] Training loss: 0.305\n",
            "[5,  1200] Validation loss: 0.351\n",
            "[5,  1400] Training loss: 0.312\n",
            "[5,  1400] Validation loss: 0.298\n",
            "[5,  1600] Training loss: 0.292\n",
            "[5,  1600] Validation loss: 0.322\n",
            "[5,  1800] Training loss: 0.318\n",
            "[5,  1800] Validation loss: 0.192\n",
            "[5,  2000] Training loss: 0.306\n",
            "[5,  2000] Validation loss: 0.861\n",
            "[5,  2200] Training loss: 0.323\n",
            "[5,  2200] Validation loss: 0.314\n",
            "[5,  2400] Training loss: 0.301\n",
            "[5,  2400] Validation loss: 0.414\n",
            "[5,  2600] Training loss: 0.314\n",
            "[5,  2600] Validation loss: 0.278\n",
            "[5,  2800] Training loss: 0.294\n",
            "[5,  2800] Validation loss: 0.202\n",
            "[5,  3000] Training loss: 0.317\n",
            "[5,  3000] Validation loss: 0.276\n",
            "[5,  3200] Training loss: 0.298\n",
            "[5,  3200] Validation loss: 0.508\n",
            "[5,  3400] Training loss: 0.288\n",
            "[5,  3400] Validation loss: 0.625\n",
            "[5,  3600] Training loss: 0.320\n",
            "[5,  3600] Validation loss: 0.250\n",
            "[5,  3800] Training loss: 0.312\n",
            "[5,  3800] Validation loss: 0.445\n",
            "[5,  4000] Training loss: 0.320\n",
            "[5,  4000] Validation loss: 0.446\n",
            "[5,  4200] Training loss: 0.311\n",
            "[5,  4200] Validation loss: 0.167\n",
            "[5,  4400] Training loss: 0.298\n",
            "[5,  4400] Validation loss: 0.450\n",
            "[5,  4600] Training loss: 0.335\n",
            "[5,  4600] Validation loss: 0.305\n",
            "[5,  4800] Training loss: 0.309\n",
            "[5,  4800] Validation loss: 0.419\n",
            "[5,  5000] Training loss: 0.308\n",
            "[5,  5000] Validation loss: 0.066\n",
            "[5,  5200] Training loss: 0.318\n",
            "[5,  5200] Validation loss: 0.148\n",
            "[5,  5400] Training loss: 0.299\n",
            "[5,  5400] Validation loss: 0.440\n",
            "[5,  5600] Training loss: 0.314\n",
            "[5,  5600] Validation loss: 0.215\n",
            "[5,  5800] Training loss: 0.280\n",
            "[5,  5800] Validation loss: 0.319\n",
            "[5,  6000] Training loss: 0.301\n",
            "[5,  6000] Validation loss: 0.315\n",
            "[5,  6200] Training loss: 0.315\n",
            "[5,  6200] Validation loss: 0.421\n",
            "[5,  6400] Training loss: 0.316\n",
            "[5,  6400] Validation loss: 0.834\n",
            "[5,  6600] Training loss: 0.313\n",
            "[5,  6600] Validation loss: 0.445\n",
            "[5,  6800] Training loss: 0.326\n",
            "[5,  6800] Validation loss: 0.310\n",
            "[5,  7000] Training loss: 0.307\n",
            "[5,  7000] Validation loss: 0.303\n",
            "[5,  7200] Training loss: 0.325\n",
            "[5,  7200] Validation loss: 0.345\n",
            "[5,  7400] Training loss: 0.298\n",
            "[5,  7400] Validation loss: 0.228\n",
            "[5,  7600] Training loss: 0.310\n",
            "[5,  7600] Validation loss: 0.230\n",
            "[5,  7800] Training loss: 0.320\n",
            "[5,  7800] Validation loss: 0.096\n",
            "[5,  8000] Training loss: 0.309\n",
            "[5,  8000] Validation loss: 0.271\n",
            "[5,  8200] Training loss: 0.314\n",
            "[5,  8200] Validation loss: 0.290\n",
            "[5,  8400] Training loss: 0.313\n",
            "[5,  8400] Validation loss: 0.404\n",
            "[5,  8600] Training loss: 0.304\n",
            "[5,  8600] Validation loss: 0.282\n",
            "[5,  8800] Training loss: 0.309\n",
            "[5,  8800] Validation loss: 0.315\n",
            "[5,  9000] Training loss: 0.304\n",
            "[5,  9000] Validation loss: 0.317\n",
            "[5,  9200] Training loss: 0.298\n",
            "[5,  9200] Validation loss: 0.202\n",
            "[5,  9400] Training loss: 0.315\n",
            "[5,  9400] Validation loss: 0.123\n",
            "[5,  9600] Training loss: 0.297\n",
            "[5,  9600] Validation loss: 0.559\n",
            "[5,  9800] Training loss: 0.294\n",
            "[5,  9800] Validation loss: 0.325\n",
            "[5, 10000] Training loss: 0.284\n",
            "[5, 10000] Validation loss: 0.488\n",
            "[5, 10200] Training loss: 0.306\n",
            "[5, 10200] Validation loss: 0.123\n",
            "[5, 10400] Training loss: 0.292\n",
            "[5, 10400] Validation loss: 0.363\n",
            "[5, 10600] Training loss: 0.301\n",
            "[5, 10600] Validation loss: 0.251\n",
            "[5, 10800] Training loss: 0.310\n",
            "[5, 10800] Validation loss: 0.605\n",
            "[5, 11000] Training loss: 0.294\n",
            "[5, 11000] Validation loss: 0.223\n",
            "[5, 11200] Training loss: 0.307\n",
            "[5, 11200] Validation loss: 0.637\n",
            "[5, 11400] Training loss: 0.321\n",
            "[5, 11400] Validation loss: 0.664\n",
            "[5, 11600] Training loss: 0.300\n",
            "[5, 11600] Validation loss: 0.536\n",
            "[5, 11800] Training loss: 0.297\n",
            "[5, 11800] Validation loss: 0.265\n",
            "[5, 12000] Training loss: 0.307\n",
            "[5, 12000] Validation loss: 0.239\n",
            "[5, 12200] Training loss: 0.308\n",
            "[5, 12200] Validation loss: 0.110\n",
            "[5, 12400] Training loss: 0.299\n",
            "[5, 12400] Validation loss: 0.529\n",
            "[5, 12600] Training loss: 0.312\n",
            "[5, 12600] Validation loss: 0.175\n",
            "[5, 12800] Training loss: 0.317\n",
            "[5, 12800] Validation loss: 0.281\n",
            "[5, 13000] Training loss: 0.311\n",
            "[5, 13000] Validation loss: 0.115\n",
            "[5, 13200] Training loss: 0.315\n",
            "[5, 13200] Validation loss: 0.133\n",
            "[5, 13400] Training loss: 0.301\n",
            "[5, 13400] Validation loss: 0.610\n",
            "[5, 13600] Training loss: 0.299\n",
            "[5, 13600] Validation loss: 0.413\n",
            "[5, 13800] Training loss: 0.302\n",
            "[5, 13800] Validation loss: 0.281\n",
            "[5, 14000] Training loss: 0.292\n",
            "[5, 14000] Validation loss: 0.322\n",
            "[5, 14200] Training loss: 0.314\n",
            "[5, 14200] Validation loss: 0.436\n",
            "[5, 14400] Training loss: 0.296\n",
            "[5, 14400] Validation loss: 0.143\n",
            "[5, 14600] Training loss: 0.308\n",
            "[5, 14600] Validation loss: 0.252\n",
            "[5, 14800] Training loss: 0.307\n",
            "[5, 14800] Validation loss: 0.363\n",
            "[5, 15000] Training loss: 0.312\n",
            "[5, 15000] Validation loss: 0.301\n",
            "[5, 15200] Training loss: 0.292\n",
            "[5, 15200] Validation loss: 0.521\n",
            "[5, 15400] Training loss: 0.294\n",
            "[5, 15400] Validation loss: 0.151\n",
            "[5, 15600] Training loss: 0.316\n",
            "[5, 15600] Validation loss: 0.373\n",
            "[5, 15800] Training loss: 0.303\n",
            "[5, 15800] Validation loss: 0.581\n",
            "[5, 16000] Training loss: 0.297\n",
            "[5, 16000] Validation loss: 0.409\n",
            "[5, 16200] Training loss: 0.316\n",
            "[5, 16200] Validation loss: 0.291\n",
            "[5, 16400] Training loss: 0.301\n",
            "[5, 16400] Validation loss: 0.300\n",
            "[5, 16600] Training loss: 0.293\n",
            "[5, 16600] Validation loss: 0.420\n",
            "[5, 16800] Training loss: 0.316\n",
            "[5, 16800] Validation loss: 0.555\n",
            "[5, 17000] Training loss: 0.310\n",
            "[5, 17000] Validation loss: 0.290\n",
            "[5, 17200] Training loss: 0.299\n",
            "[5, 17200] Validation loss: 0.394\n",
            "[5, 17400] Training loss: 0.314\n",
            "[5, 17400] Validation loss: 0.612\n",
            "[5, 17600] Training loss: 0.327\n",
            "[5, 17600] Validation loss: 0.341\n",
            "[5, 17800] Training loss: 0.314\n",
            "[5, 17800] Validation loss: 0.142\n",
            "[5, 18000] Training loss: 0.296\n",
            "[5, 18000] Validation loss: 0.218\n",
            "[5, 18200] Training loss: 0.306\n",
            "[5, 18200] Validation loss: 0.324\n",
            "[5, 18400] Training loss: 0.306\n",
            "[5, 18400] Validation loss: 0.609\n",
            "[5, 18600] Training loss: 0.308\n",
            "[5, 18600] Validation loss: 0.185\n",
            "[6,   200] Training loss: 0.307\n",
            "[6,   200] Validation loss: 0.168\n",
            "[6,   400] Training loss: 0.281\n",
            "[6,   400] Validation loss: 0.238\n",
            "[6,   600] Training loss: 0.286\n",
            "[6,   600] Validation loss: 0.206\n",
            "[6,   800] Training loss: 0.282\n",
            "[6,   800] Validation loss: 0.147\n",
            "[6,  1000] Training loss: 0.282\n",
            "[6,  1000] Validation loss: 0.169\n",
            "[6,  1200] Training loss: 0.280\n",
            "[6,  1200] Validation loss: 0.472\n",
            "[6,  1400] Training loss: 0.305\n",
            "[6,  1400] Validation loss: 0.338\n",
            "[6,  1600] Training loss: 0.284\n",
            "[6,  1600] Validation loss: 0.429\n",
            "[6,  1800] Training loss: 0.286\n",
            "[6,  1800] Validation loss: 0.205\n",
            "[6,  2000] Training loss: 0.278\n",
            "[6,  2000] Validation loss: 0.238\n",
            "[6,  2200] Training loss: 0.288\n",
            "[6,  2200] Validation loss: 0.478\n",
            "[6,  2400] Training loss: 0.291\n",
            "[6,  2400] Validation loss: 0.421\n",
            "[6,  2600] Training loss: 0.297\n",
            "[6,  2600] Validation loss: 0.365\n",
            "[6,  2800] Training loss: 0.275\n",
            "[6,  2800] Validation loss: 0.150\n",
            "[6,  3000] Training loss: 0.291\n",
            "[6,  3000] Validation loss: 0.446\n",
            "[6,  3200] Training loss: 0.297\n",
            "[6,  3200] Validation loss: 0.223\n",
            "[6,  3400] Training loss: 0.292\n",
            "[6,  3400] Validation loss: 0.388\n",
            "[6,  3600] Training loss: 0.304\n",
            "[6,  3600] Validation loss: 0.106\n",
            "[6,  3800] Training loss: 0.290\n",
            "[6,  3800] Validation loss: 0.126\n",
            "[6,  4000] Training loss: 0.310\n",
            "[6,  4000] Validation loss: 0.281\n",
            "[6,  4200] Training loss: 0.304\n",
            "[6,  4200] Validation loss: 0.501\n",
            "[6,  4400] Training loss: 0.308\n",
            "[6,  4400] Validation loss: 0.693\n",
            "[6,  4600] Training loss: 0.305\n",
            "[6,  4600] Validation loss: 0.150\n",
            "[6,  4800] Training loss: 0.297\n",
            "[6,  4800] Validation loss: 0.217\n",
            "[6,  5000] Training loss: 0.308\n",
            "[6,  5000] Validation loss: 0.240\n",
            "[6,  5200] Training loss: 0.297\n",
            "[6,  5200] Validation loss: 0.131\n",
            "[6,  5400] Training loss: 0.288\n",
            "[6,  5400] Validation loss: 0.372\n",
            "[6,  5600] Training loss: 0.279\n",
            "[6,  5600] Validation loss: 0.056\n",
            "[6,  5800] Training loss: 0.288\n",
            "[6,  5800] Validation loss: 0.058\n",
            "[6,  6000] Training loss: 0.322\n",
            "[6,  6000] Validation loss: 0.305\n",
            "[6,  6200] Training loss: 0.311\n",
            "[6,  6200] Validation loss: 0.299\n",
            "[6,  6400] Training loss: 0.302\n",
            "[6,  6400] Validation loss: 0.156\n",
            "[6,  6600] Training loss: 0.298\n",
            "[6,  6600] Validation loss: 0.198\n",
            "[6,  6800] Training loss: 0.275\n",
            "[6,  6800] Validation loss: 0.217\n",
            "[6,  7000] Training loss: 0.296\n",
            "[6,  7000] Validation loss: 0.413\n",
            "[6,  7200] Training loss: 0.293\n",
            "[6,  7200] Validation loss: 0.275\n",
            "[6,  7400] Training loss: 0.295\n",
            "[6,  7400] Validation loss: 0.262\n",
            "[6,  7600] Training loss: 0.286\n",
            "[6,  7600] Validation loss: 0.469\n",
            "[6,  7800] Training loss: 0.305\n",
            "[6,  7800] Validation loss: 0.295\n",
            "[6,  8000] Training loss: 0.295\n",
            "[6,  8000] Validation loss: 0.106\n",
            "[6,  8200] Training loss: 0.309\n",
            "[6,  8200] Validation loss: 0.383\n",
            "[6,  8400] Training loss: 0.294\n",
            "[6,  8400] Validation loss: 0.205\n",
            "[6,  8600] Training loss: 0.284\n",
            "[6,  8600] Validation loss: 0.231\n",
            "[6,  8800] Training loss: 0.307\n",
            "[6,  8800] Validation loss: 0.298\n",
            "[6,  9000] Training loss: 0.283\n",
            "[6,  9000] Validation loss: 0.144\n",
            "[6,  9200] Training loss: 0.299\n",
            "[6,  9200] Validation loss: 0.292\n",
            "[6,  9400] Training loss: 0.287\n",
            "[6,  9400] Validation loss: 0.324\n",
            "[6,  9600] Training loss: 0.287\n",
            "[6,  9600] Validation loss: 0.813\n",
            "[6,  9800] Training loss: 0.291\n",
            "[6,  9800] Validation loss: 0.244\n",
            "[6, 10000] Training loss: 0.298\n",
            "[6, 10000] Validation loss: 0.265\n",
            "[6, 10200] Training loss: 0.295\n",
            "[6, 10200] Validation loss: 0.177\n",
            "[6, 10400] Training loss: 0.300\n",
            "[6, 10400] Validation loss: 0.539\n",
            "[6, 10600] Training loss: 0.297\n",
            "[6, 10600] Validation loss: 0.433\n",
            "[6, 10800] Training loss: 0.294\n",
            "[6, 10800] Validation loss: 0.122\n",
            "[6, 11000] Training loss: 0.294\n",
            "[6, 11000] Validation loss: 0.210\n",
            "[6, 11200] Training loss: 0.306\n",
            "[6, 11200] Validation loss: 0.242\n",
            "[6, 11400] Training loss: 0.282\n",
            "[6, 11400] Validation loss: 0.182\n",
            "[6, 11600] Training loss: 0.306\n",
            "[6, 11600] Validation loss: 0.203\n",
            "[6, 11800] Training loss: 0.300\n",
            "[6, 11800] Validation loss: 0.355\n",
            "[6, 12000] Training loss: 0.301\n",
            "[6, 12000] Validation loss: 0.629\n",
            "[6, 12200] Training loss: 0.277\n",
            "[6, 12200] Validation loss: 0.239\n",
            "[6, 12400] Training loss: 0.280\n",
            "[6, 12400] Validation loss: 0.233\n",
            "[6, 12600] Training loss: 0.294\n",
            "[6, 12600] Validation loss: 0.385\n",
            "[6, 12800] Training loss: 0.282\n",
            "[6, 12800] Validation loss: 0.255\n",
            "[6, 13000] Training loss: 0.301\n",
            "[6, 13000] Validation loss: 0.203\n",
            "[6, 13200] Training loss: 0.295\n",
            "[6, 13200] Validation loss: 0.230\n",
            "[6, 13400] Training loss: 0.298\n",
            "[6, 13400] Validation loss: 0.217\n",
            "[6, 13600] Training loss: 0.308\n",
            "[6, 13600] Validation loss: 0.394\n",
            "[6, 13800] Training loss: 0.288\n",
            "[6, 13800] Validation loss: 0.500\n",
            "[6, 14000] Training loss: 0.278\n",
            "[6, 14000] Validation loss: 0.256\n",
            "[6, 14200] Training loss: 0.285\n",
            "[6, 14200] Validation loss: 0.261\n",
            "[6, 14400] Training loss: 0.288\n",
            "[6, 14400] Validation loss: 0.153\n",
            "[6, 14600] Training loss: 0.288\n",
            "[6, 14600] Validation loss: 0.420\n",
            "[6, 14800] Training loss: 0.300\n",
            "[6, 14800] Validation loss: 0.239\n",
            "[6, 15000] Training loss: 0.310\n",
            "[6, 15000] Validation loss: 0.497\n",
            "[6, 15200] Training loss: 0.285\n",
            "[6, 15200] Validation loss: 0.311\n",
            "[6, 15400] Training loss: 0.297\n",
            "[6, 15400] Validation loss: 0.444\n",
            "[6, 15600] Training loss: 0.302\n",
            "[6, 15600] Validation loss: 0.085\n",
            "[6, 15800] Training loss: 0.292\n",
            "[6, 15800] Validation loss: 0.337\n",
            "[6, 16000] Training loss: 0.308\n",
            "[6, 16000] Validation loss: 0.299\n",
            "[6, 16200] Training loss: 0.292\n",
            "[6, 16200] Validation loss: 0.192\n",
            "[6, 16400] Training loss: 0.306\n",
            "[6, 16400] Validation loss: 0.156\n",
            "[6, 16600] Training loss: 0.282\n",
            "[6, 16600] Validation loss: 0.286\n",
            "[6, 16800] Training loss: 0.297\n",
            "[6, 16800] Validation loss: 0.256\n",
            "[6, 17000] Training loss: 0.291\n",
            "[6, 17000] Validation loss: 0.404\n",
            "[6, 17200] Training loss: 0.290\n",
            "[6, 17200] Validation loss: 0.245\n",
            "[6, 17400] Training loss: 0.300\n",
            "[6, 17400] Validation loss: 0.393\n",
            "[6, 17600] Training loss: 0.278\n",
            "[6, 17600] Validation loss: 0.166\n",
            "[6, 17800] Training loss: 0.296\n",
            "[6, 17800] Validation loss: 0.137\n",
            "[6, 18000] Training loss: 0.284\n",
            "[6, 18000] Validation loss: 0.529\n",
            "[6, 18200] Training loss: 0.276\n",
            "[6, 18200] Validation loss: 0.239\n",
            "[6, 18400] Training loss: 0.300\n",
            "[6, 18400] Validation loss: 0.370\n",
            "[6, 18600] Training loss: 0.301\n",
            "[6, 18600] Validation loss: 0.553\n",
            "[7,   200] Training loss: 0.280\n",
            "[7,   200] Validation loss: 0.316\n",
            "[7,   400] Training loss: 0.284\n",
            "[7,   400] Validation loss: 0.107\n",
            "[7,   600] Training loss: 0.284\n",
            "[7,   600] Validation loss: 0.392\n",
            "[7,   800] Training loss: 0.288\n",
            "[7,   800] Validation loss: 0.186\n",
            "[7,  1000] Training loss: 0.275\n",
            "[7,  1000] Validation loss: 0.150\n",
            "[7,  1200] Training loss: 0.286\n",
            "[7,  1200] Validation loss: 0.370\n",
            "[7,  1400] Training loss: 0.292\n",
            "[7,  1400] Validation loss: 0.128\n",
            "[7,  1600] Training loss: 0.300\n",
            "[7,  1600] Validation loss: 0.372\n",
            "[7,  1800] Training loss: 0.283\n",
            "[7,  1800] Validation loss: 0.071\n",
            "[7,  2000] Training loss: 0.292\n",
            "[7,  2000] Validation loss: 0.206\n",
            "[7,  2200] Training loss: 0.287\n",
            "[7,  2200] Validation loss: 0.289\n",
            "[7,  2400] Training loss: 0.299\n",
            "[7,  2400] Validation loss: 0.190\n",
            "[7,  2600] Training loss: 0.276\n",
            "[7,  2600] Validation loss: 0.445\n",
            "[7,  2800] Training loss: 0.284\n",
            "[7,  2800] Validation loss: 0.226\n",
            "[7,  3000] Training loss: 0.271\n",
            "[7,  3000] Validation loss: 0.378\n",
            "[7,  3200] Training loss: 0.293\n",
            "[7,  3200] Validation loss: 0.415\n",
            "[7,  3400] Training loss: 0.287\n",
            "[7,  3400] Validation loss: 0.187\n",
            "[7,  3600] Training loss: 0.286\n",
            "[7,  3600] Validation loss: 0.188\n",
            "[7,  3800] Training loss: 0.294\n",
            "[7,  3800] Validation loss: 0.255\n",
            "[7,  4000] Training loss: 0.278\n",
            "[7,  4000] Validation loss: 0.454\n",
            "[7,  4200] Training loss: 0.282\n",
            "[7,  4200] Validation loss: 0.259\n",
            "[7,  4400] Training loss: 0.300\n",
            "[7,  4400] Validation loss: 0.317\n",
            "[7,  4600] Training loss: 0.284\n",
            "[7,  4600] Validation loss: 0.245\n",
            "[7,  4800] Training loss: 0.293\n",
            "[7,  4800] Validation loss: 0.355\n",
            "[7,  5000] Training loss: 0.277\n",
            "[7,  5000] Validation loss: 0.094\n",
            "[7,  5200] Training loss: 0.294\n",
            "[7,  5200] Validation loss: 0.164\n",
            "[7,  5400] Training loss: 0.271\n",
            "[7,  5400] Validation loss: 0.168\n",
            "[7,  5600] Training loss: 0.270\n",
            "[7,  5600] Validation loss: 0.398\n",
            "[7,  5800] Training loss: 0.279\n",
            "[7,  5800] Validation loss: 0.385\n",
            "[7,  6000] Training loss: 0.299\n",
            "[7,  6000] Validation loss: 0.494\n",
            "[7,  6200] Training loss: 0.296\n",
            "[7,  6200] Validation loss: 0.255\n",
            "[7,  6400] Training loss: 0.278\n",
            "[7,  6400] Validation loss: 0.212\n",
            "[7,  6600] Training loss: 0.279\n",
            "[7,  6600] Validation loss: 0.491\n",
            "[7,  6800] Training loss: 0.286\n",
            "[7,  6800] Validation loss: 0.160\n",
            "[7,  7000] Training loss: 0.290\n",
            "[7,  7000] Validation loss: 0.253\n",
            "[7,  7200] Training loss: 0.274\n",
            "[7,  7200] Validation loss: 0.317\n",
            "[7,  7400] Training loss: 0.293\n",
            "[7,  7400] Validation loss: 0.517\n",
            "[7,  7600] Training loss: 0.281\n",
            "[7,  7600] Validation loss: 0.469\n",
            "[7,  7800] Training loss: 0.294\n",
            "[7,  7800] Validation loss: 0.265\n",
            "[7,  8000] Training loss: 0.281\n",
            "[7,  8000] Validation loss: 0.072\n",
            "[7,  8200] Training loss: 0.269\n",
            "[7,  8200] Validation loss: 0.167\n",
            "[7,  8400] Training loss: 0.274\n",
            "[7,  8400] Validation loss: 0.256\n",
            "[7,  8600] Training loss: 0.296\n",
            "[7,  8600] Validation loss: 0.473\n",
            "[7,  8800] Training loss: 0.284\n",
            "[7,  8800] Validation loss: 0.462\n",
            "[7,  9000] Training loss: 0.283\n",
            "[7,  9000] Validation loss: 0.557\n",
            "[7,  9200] Training loss: 0.277\n",
            "[7,  9200] Validation loss: 0.431\n",
            "[7,  9400] Training loss: 0.279\n",
            "[7,  9400] Validation loss: 0.473\n",
            "[7,  9600] Training loss: 0.282\n",
            "[7,  9600] Validation loss: 0.258\n",
            "[7,  9800] Training loss: 0.303\n",
            "[7,  9800] Validation loss: 0.286\n",
            "[7, 10000] Training loss: 0.272\n",
            "[7, 10000] Validation loss: 0.635\n",
            "[7, 10200] Training loss: 0.276\n",
            "[7, 10200] Validation loss: 0.127\n",
            "[7, 10400] Training loss: 0.299\n",
            "[7, 10400] Validation loss: 0.135\n",
            "[7, 10600] Training loss: 0.285\n",
            "[7, 10600] Validation loss: 0.123\n",
            "[7, 10800] Training loss: 0.273\n",
            "[7, 10800] Validation loss: 0.336\n",
            "[7, 11000] Training loss: 0.265\n",
            "[7, 11000] Validation loss: 0.291\n",
            "[7, 11200] Training loss: 0.278\n",
            "[7, 11200] Validation loss: 0.144\n",
            "[7, 11400] Training loss: 0.280\n",
            "[7, 11400] Validation loss: 0.309\n",
            "[7, 11600] Training loss: 0.288\n",
            "[7, 11600] Validation loss: 0.202\n",
            "[7, 11800] Training loss: 0.285\n",
            "[7, 11800] Validation loss: 0.379\n",
            "[7, 12000] Training loss: 0.287\n",
            "[7, 12000] Validation loss: 0.546\n",
            "[7, 12200] Training loss: 0.269\n",
            "[7, 12200] Validation loss: 0.590\n",
            "[7, 12400] Training loss: 0.286\n",
            "[7, 12400] Validation loss: 0.126\n",
            "[7, 12600] Training loss: 0.288\n",
            "[7, 12600] Validation loss: 0.205\n",
            "[7, 12800] Training loss: 0.282\n",
            "[7, 12800] Validation loss: 0.239\n",
            "[7, 13000] Training loss: 0.282\n",
            "[7, 13000] Validation loss: 0.327\n",
            "[7, 13200] Training loss: 0.286\n",
            "[7, 13200] Validation loss: 0.203\n",
            "[7, 13400] Training loss: 0.287\n",
            "[7, 13400] Validation loss: 0.474\n",
            "[7, 13600] Training loss: 0.273\n",
            "[7, 13600] Validation loss: 0.442\n",
            "[7, 13800] Training loss: 0.298\n",
            "[7, 13800] Validation loss: 0.235\n",
            "[7, 14000] Training loss: 0.269\n",
            "[7, 14000] Validation loss: 0.371\n",
            "[7, 14200] Training loss: 0.293\n",
            "[7, 14200] Validation loss: 0.209\n",
            "[7, 14400] Training loss: 0.287\n",
            "[7, 14400] Validation loss: 0.429\n",
            "[7, 14600] Training loss: 0.273\n",
            "[7, 14600] Validation loss: 0.266\n",
            "[7, 14800] Training loss: 0.288\n",
            "[7, 14800] Validation loss: 0.229\n",
            "[7, 15000] Training loss: 0.284\n",
            "[7, 15000] Validation loss: 0.099\n",
            "[7, 15200] Training loss: 0.281\n",
            "[7, 15200] Validation loss: 0.226\n",
            "[7, 15400] Training loss: 0.281\n",
            "[7, 15400] Validation loss: 0.249\n",
            "[7, 15600] Training loss: 0.276\n",
            "[7, 15600] Validation loss: 0.327\n",
            "[7, 15800] Training loss: 0.288\n",
            "[7, 15800] Validation loss: 0.423\n",
            "[7, 16000] Training loss: 0.296\n",
            "[7, 16000] Validation loss: 0.124\n",
            "[7, 16200] Training loss: 0.271\n",
            "[7, 16200] Validation loss: 0.231\n",
            "[7, 16400] Training loss: 0.280\n",
            "[7, 16400] Validation loss: 0.480\n",
            "[7, 16600] Training loss: 0.274\n",
            "[7, 16600] Validation loss: 0.293\n",
            "[7, 16800] Training loss: 0.281\n",
            "[7, 16800] Validation loss: 0.252\n",
            "[7, 17000] Training loss: 0.299\n",
            "[7, 17000] Validation loss: 0.267\n",
            "[7, 17200] Training loss: 0.294\n",
            "[7, 17200] Validation loss: 0.814\n",
            "[7, 17400] Training loss: 0.279\n",
            "[7, 17400] Validation loss: 0.558\n",
            "[7, 17600] Training loss: 0.283\n",
            "[7, 17600] Validation loss: 0.257\n",
            "[7, 17800] Training loss: 0.285\n",
            "[7, 17800] Validation loss: 0.556\n",
            "[7, 18000] Training loss: 0.292\n",
            "[7, 18000] Validation loss: 0.174\n",
            "[7, 18200] Training loss: 0.277\n",
            "[7, 18200] Validation loss: 0.228\n",
            "[7, 18400] Training loss: 0.281\n",
            "[7, 18400] Validation loss: 0.414\n",
            "[7, 18600] Training loss: 0.269\n",
            "[7, 18600] Validation loss: 0.224\n",
            "[8,   200] Training loss: 0.261\n",
            "[8,   200] Validation loss: 0.361\n",
            "[8,   400] Training loss: 0.261\n",
            "[8,   400] Validation loss: 0.227\n",
            "[8,   600] Training loss: 0.274\n",
            "[8,   600] Validation loss: 0.270\n",
            "[8,   800] Training loss: 0.280\n",
            "[8,   800] Validation loss: 0.177\n",
            "[8,  1000] Training loss: 0.284\n",
            "[8,  1000] Validation loss: 0.316\n",
            "[8,  1200] Training loss: 0.277\n",
            "[8,  1200] Validation loss: 0.228\n",
            "[8,  1400] Training loss: 0.270\n",
            "[8,  1400] Validation loss: 0.276\n",
            "[8,  1600] Training loss: 0.276\n",
            "[8,  1600] Validation loss: 0.182\n",
            "[8,  1800] Training loss: 0.272\n",
            "[8,  1800] Validation loss: 0.216\n",
            "[8,  2000] Training loss: 0.283\n",
            "[8,  2000] Validation loss: 0.501\n",
            "[8,  2200] Training loss: 0.284\n",
            "[8,  2200] Validation loss: 0.451\n",
            "[8,  2400] Training loss: 0.284\n",
            "[8,  2400] Validation loss: 0.248\n",
            "[8,  2600] Training loss: 0.276\n",
            "[8,  2600] Validation loss: 0.260\n",
            "[8,  2800] Training loss: 0.282\n",
            "[8,  2800] Validation loss: 0.537\n",
            "[8,  3000] Training loss: 0.286\n",
            "[8,  3000] Validation loss: 0.249\n",
            "[8,  3200] Training loss: 0.280\n",
            "[8,  3200] Validation loss: 0.231\n",
            "[8,  3400] Training loss: 0.283\n",
            "[8,  3400] Validation loss: 0.136\n",
            "[8,  3600] Training loss: 0.277\n",
            "[8,  3600] Validation loss: 0.486\n",
            "[8,  3800] Training loss: 0.267\n",
            "[8,  3800] Validation loss: 0.236\n",
            "[8,  4000] Training loss: 0.290\n",
            "[8,  4000] Validation loss: 0.288\n",
            "[8,  4200] Training loss: 0.289\n",
            "[8,  4200] Validation loss: 0.112\n",
            "[8,  4400] Training loss: 0.278\n",
            "[8,  4400] Validation loss: 0.115\n",
            "[8,  4600] Training loss: 0.281\n",
            "[8,  4600] Validation loss: 0.133\n",
            "[8,  4800] Training loss: 0.270\n",
            "[8,  4800] Validation loss: 0.330\n",
            "[8,  5000] Training loss: 0.272\n",
            "[8,  5000] Validation loss: 0.271\n",
            "[8,  5200] Training loss: 0.287\n",
            "[8,  5200] Validation loss: 0.472\n",
            "[8,  5400] Training loss: 0.271\n",
            "[8,  5400] Validation loss: 0.179\n",
            "[8,  5600] Training loss: 0.265\n",
            "[8,  5600] Validation loss: 0.402\n",
            "[8,  5800] Training loss: 0.280\n",
            "[8,  5800] Validation loss: 0.279\n",
            "[8,  6000] Training loss: 0.277\n",
            "[8,  6000] Validation loss: 0.294\n",
            "[8,  6200] Training loss: 0.271\n",
            "[8,  6200] Validation loss: 0.357\n",
            "[8,  6400] Training loss: 0.261\n",
            "[8,  6400] Validation loss: 0.264\n",
            "[8,  6600] Training loss: 0.285\n",
            "[8,  6600] Validation loss: 0.482\n",
            "[8,  6800] Training loss: 0.274\n",
            "[8,  6800] Validation loss: 0.118\n",
            "[8,  7000] Training loss: 0.269\n",
            "[8,  7000] Validation loss: 0.131\n",
            "[8,  7200] Training loss: 0.288\n",
            "[8,  7200] Validation loss: 0.099\n",
            "[8,  7400] Training loss: 0.271\n",
            "[8,  7400] Validation loss: 0.142\n",
            "[8,  7600] Training loss: 0.287\n",
            "[8,  7600] Validation loss: 0.309\n",
            "[8,  7800] Training loss: 0.261\n",
            "[8,  7800] Validation loss: 0.241\n",
            "[8,  8000] Training loss: 0.265\n",
            "[8,  8000] Validation loss: 0.450\n",
            "[8,  8200] Training loss: 0.281\n",
            "[8,  8200] Validation loss: 0.239\n",
            "[8,  8400] Training loss: 0.275\n",
            "[8,  8400] Validation loss: 0.161\n",
            "[8,  8600] Training loss: 0.280\n",
            "[8,  8600] Validation loss: 0.151\n",
            "[8,  8800] Training loss: 0.265\n",
            "[8,  8800] Validation loss: 0.369\n",
            "[8,  9000] Training loss: 0.270\n",
            "[8,  9000] Validation loss: 0.423\n",
            "[8,  9200] Training loss: 0.271\n",
            "[8,  9200] Validation loss: 0.565\n",
            "[8,  9400] Training loss: 0.278\n",
            "[8,  9400] Validation loss: 0.228\n",
            "[8,  9600] Training loss: 0.264\n",
            "[8,  9600] Validation loss: 0.230\n",
            "[8,  9800] Training loss: 0.280\n",
            "[8,  9800] Validation loss: 0.105\n",
            "[8, 10000] Training loss: 0.268\n",
            "[8, 10000] Validation loss: 0.236\n",
            "[8, 10200] Training loss: 0.270\n",
            "[8, 10200] Validation loss: 0.319\n",
            "[8, 10400] Training loss: 0.279\n",
            "[8, 10400] Validation loss: 0.197\n",
            "[8, 10600] Training loss: 0.269\n",
            "[8, 10600] Validation loss: 0.317\n",
            "[8, 10800] Training loss: 0.276\n",
            "[8, 10800] Validation loss: 0.205\n",
            "[8, 11000] Training loss: 0.281\n",
            "[8, 11000] Validation loss: 0.236\n",
            "[8, 11200] Training loss: 0.266\n",
            "[8, 11200] Validation loss: 0.353\n",
            "[8, 11400] Training loss: 0.284\n",
            "[8, 11400] Validation loss: 0.301\n",
            "[8, 11600] Training loss: 0.280\n",
            "[8, 11600] Validation loss: 0.296\n",
            "[8, 11800] Training loss: 0.284\n",
            "[8, 11800] Validation loss: 0.345\n",
            "[8, 12000] Training loss: 0.272\n",
            "[8, 12000] Validation loss: 0.367\n",
            "[8, 12200] Training loss: 0.242\n",
            "[8, 12200] Validation loss: 0.545\n",
            "[8, 12400] Training loss: 0.280\n",
            "[8, 12400] Validation loss: 0.364\n",
            "[8, 12600] Training loss: 0.261\n",
            "[8, 12600] Validation loss: 0.344\n",
            "[8, 12800] Training loss: 0.271\n",
            "[8, 12800] Validation loss: 0.201\n",
            "[8, 13000] Training loss: 0.285\n",
            "[8, 13000] Validation loss: 0.274\n",
            "[8, 13200] Training loss: 0.259\n",
            "[8, 13200] Validation loss: 0.379\n",
            "[8, 13400] Training loss: 0.286\n",
            "[8, 13400] Validation loss: 0.438\n",
            "[8, 13600] Training loss: 0.294\n",
            "[8, 13600] Validation loss: 0.225\n",
            "[8, 13800] Training loss: 0.270\n",
            "[8, 13800] Validation loss: 0.101\n",
            "[8, 14000] Training loss: 0.270\n",
            "[8, 14000] Validation loss: 0.148\n",
            "[8, 14200] Training loss: 0.272\n",
            "[8, 14200] Validation loss: 0.153\n",
            "[8, 14400] Training loss: 0.275\n",
            "[8, 14400] Validation loss: 0.481\n",
            "[8, 14600] Training loss: 0.283\n",
            "[8, 14600] Validation loss: 0.677\n",
            "[8, 14800] Training loss: 0.268\n",
            "[8, 14800] Validation loss: 0.249\n",
            "[8, 15000] Training loss: 0.271\n",
            "[8, 15000] Validation loss: 0.813\n",
            "[8, 15200] Training loss: 0.273\n",
            "[8, 15200] Validation loss: 0.371\n",
            "[8, 15400] Training loss: 0.271\n",
            "[8, 15400] Validation loss: 0.324\n",
            "[8, 15600] Training loss: 0.264\n",
            "[8, 15600] Validation loss: 0.160\n",
            "[8, 15800] Training loss: 0.284\n",
            "[8, 15800] Validation loss: 0.525\n",
            "[8, 16000] Training loss: 0.282\n",
            "[8, 16000] Validation loss: 0.704\n",
            "[8, 16200] Training loss: 0.271\n",
            "[8, 16200] Validation loss: 0.095\n",
            "[8, 16400] Training loss: 0.280\n",
            "[8, 16400] Validation loss: 0.084\n",
            "[8, 16600] Training loss: 0.270\n",
            "[8, 16600] Validation loss: 0.244\n",
            "[8, 16800] Training loss: 0.283\n",
            "[8, 16800] Validation loss: 0.131\n",
            "[8, 17000] Training loss: 0.271\n",
            "[8, 17000] Validation loss: 0.577\n",
            "[8, 17200] Training loss: 0.278\n",
            "[8, 17200] Validation loss: 0.380\n",
            "[8, 17400] Training loss: 0.278\n",
            "[8, 17400] Validation loss: 0.100\n",
            "[8, 17600] Training loss: 0.277\n",
            "[8, 17600] Validation loss: 0.416\n",
            "[8, 17800] Training loss: 0.279\n",
            "[8, 17800] Validation loss: 0.543\n",
            "[8, 18000] Training loss: 0.263\n",
            "[8, 18000] Validation loss: 0.111\n",
            "[8, 18200] Training loss: 0.272\n",
            "[8, 18200] Validation loss: 0.125\n",
            "[8, 18400] Training loss: 0.293\n",
            "[8, 18400] Validation loss: 0.225\n",
            "[8, 18600] Training loss: 0.269\n",
            "[8, 18600] Validation loss: 0.689\n",
            "[9,   200] Training loss: 0.272\n",
            "[9,   200] Validation loss: 0.610\n",
            "[9,   400] Training loss: 0.298\n",
            "[9,   400] Validation loss: 0.449\n",
            "[9,   600] Training loss: 0.267\n",
            "[9,   600] Validation loss: 0.244\n",
            "[9,   800] Training loss: 0.275\n",
            "[9,   800] Validation loss: 0.339\n",
            "[9,  1000] Training loss: 0.282\n",
            "[9,  1000] Validation loss: 0.246\n",
            "[9,  1200] Training loss: 0.280\n",
            "[9,  1200] Validation loss: 0.177\n",
            "[9,  1400] Training loss: 0.276\n",
            "[9,  1400] Validation loss: 0.303\n",
            "[9,  1600] Training loss: 0.254\n",
            "[9,  1600] Validation loss: 0.414\n",
            "[9,  1800] Training loss: 0.268\n",
            "[9,  1800] Validation loss: 0.169\n",
            "[9,  2000] Training loss: 0.263\n",
            "[9,  2000] Validation loss: 0.124\n",
            "[9,  2200] Training loss: 0.260\n",
            "[9,  2200] Validation loss: 0.382\n",
            "[9,  2400] Training loss: 0.257\n",
            "[9,  2400] Validation loss: 0.073\n",
            "[9,  2600] Training loss: 0.266\n",
            "[9,  2600] Validation loss: 0.085\n",
            "[9,  2800] Training loss: 0.266\n",
            "[9,  2800] Validation loss: 0.302\n",
            "[9,  3000] Training loss: 0.259\n",
            "[9,  3000] Validation loss: 0.277\n",
            "[9,  3200] Training loss: 0.275\n",
            "[9,  3200] Validation loss: 0.355\n",
            "[9,  3400] Training loss: 0.271\n",
            "[9,  3400] Validation loss: 0.141\n",
            "[9,  3600] Training loss: 0.259\n",
            "[9,  3600] Validation loss: 0.437\n",
            "[9,  3800] Training loss: 0.275\n",
            "[9,  3800] Validation loss: 0.166\n",
            "[9,  4000] Training loss: 0.256\n",
            "[9,  4000] Validation loss: 0.234\n",
            "[9,  4200] Training loss: 0.273\n",
            "[9,  4200] Validation loss: 0.129\n",
            "[9,  4400] Training loss: 0.258\n",
            "[9,  4400] Validation loss: 0.475\n",
            "[9,  4600] Training loss: 0.271\n",
            "[9,  4600] Validation loss: 0.411\n",
            "[9,  4800] Training loss: 0.264\n",
            "[9,  4800] Validation loss: 0.269\n",
            "[9,  5000] Training loss: 0.269\n",
            "[9,  5000] Validation loss: 0.185\n",
            "[9,  5200] Training loss: 0.257\n",
            "[9,  5200] Validation loss: 0.298\n",
            "[9,  5400] Training loss: 0.265\n",
            "[9,  5400] Validation loss: 0.265\n",
            "[9,  5600] Training loss: 0.285\n",
            "[9,  5600] Validation loss: 0.303\n",
            "[9,  5800] Training loss: 0.259\n",
            "[9,  5800] Validation loss: 0.336\n",
            "[9,  6000] Training loss: 0.248\n",
            "[9,  6000] Validation loss: 0.495\n",
            "[9,  6200] Training loss: 0.269\n",
            "[9,  6200] Validation loss: 0.067\n",
            "[9,  6400] Training loss: 0.276\n",
            "[9,  6400] Validation loss: 0.245\n",
            "[9,  6600] Training loss: 0.276\n",
            "[9,  6600] Validation loss: 0.037\n",
            "[9,  6800] Training loss: 0.267\n",
            "[9,  6800] Validation loss: 0.297\n",
            "[9,  7000] Training loss: 0.286\n",
            "[9,  7000] Validation loss: 0.297\n",
            "[9,  7200] Training loss: 0.263\n",
            "[9,  7200] Validation loss: 0.134\n",
            "[9,  7400] Training loss: 0.262\n",
            "[9,  7400] Validation loss: 0.225\n",
            "[9,  7600] Training loss: 0.266\n",
            "[9,  7600] Validation loss: 0.282\n",
            "[9,  7800] Training loss: 0.269\n",
            "[9,  7800] Validation loss: 0.124\n",
            "[9,  8000] Training loss: 0.253\n",
            "[9,  8000] Validation loss: 0.203\n",
            "[9,  8200] Training loss: 0.267\n",
            "[9,  8200] Validation loss: 0.092\n",
            "[9,  8400] Training loss: 0.264\n",
            "[9,  8400] Validation loss: 0.154\n",
            "[9,  8600] Training loss: 0.271\n",
            "[9,  8600] Validation loss: 0.121\n",
            "[9,  8800] Training loss: 0.263\n",
            "[9,  8800] Validation loss: 0.299\n",
            "[9,  9000] Training loss: 0.283\n",
            "[9,  9000] Validation loss: 0.287\n",
            "[9,  9200] Training loss: 0.249\n",
            "[9,  9200] Validation loss: 0.208\n",
            "[9,  9400] Training loss: 0.267\n",
            "[9,  9400] Validation loss: 0.301\n",
            "[9,  9600] Training loss: 0.262\n",
            "[9,  9600] Validation loss: 0.452\n",
            "[9,  9800] Training loss: 0.287\n",
            "[9,  9800] Validation loss: 0.174\n",
            "[9, 10000] Training loss: 0.280\n",
            "[9, 10000] Validation loss: 0.391\n",
            "[9, 10200] Training loss: 0.247\n",
            "[9, 10200] Validation loss: 0.382\n",
            "[9, 10400] Training loss: 0.273\n",
            "[9, 10400] Validation loss: 0.336\n",
            "[9, 10600] Training loss: 0.273\n",
            "[9, 10600] Validation loss: 0.156\n",
            "[9, 10800] Training loss: 0.276\n",
            "[9, 10800] Validation loss: 0.329\n",
            "[9, 11000] Training loss: 0.272\n",
            "[9, 11000] Validation loss: 0.378\n",
            "[9, 11200] Training loss: 0.275\n",
            "[9, 11200] Validation loss: 0.198\n",
            "[9, 11400] Training loss: 0.276\n",
            "[9, 11400] Validation loss: 0.172\n",
            "[9, 11600] Training loss: 0.258\n",
            "[9, 11600] Validation loss: 0.325\n",
            "[9, 11800] Training loss: 0.265\n",
            "[9, 11800] Validation loss: 0.410\n",
            "[9, 12000] Training loss: 0.284\n",
            "[9, 12000] Validation loss: 0.126\n",
            "[9, 12200] Training loss: 0.264\n",
            "[9, 12200] Validation loss: 0.266\n",
            "[9, 12400] Training loss: 0.266\n",
            "[9, 12400] Validation loss: 0.518\n",
            "[9, 12600] Training loss: 0.262\n",
            "[9, 12600] Validation loss: 0.291\n",
            "[9, 12800] Training loss: 0.276\n",
            "[9, 12800] Validation loss: 0.333\n",
            "[9, 13000] Training loss: 0.261\n",
            "[9, 13000] Validation loss: 0.277\n",
            "[9, 13200] Training loss: 0.272\n",
            "[9, 13200] Validation loss: 0.135\n",
            "[9, 13400] Training loss: 0.274\n",
            "[9, 13400] Validation loss: 0.280\n",
            "[9, 13600] Training loss: 0.253\n",
            "[9, 13600] Validation loss: 0.197\n",
            "[9, 13800] Training loss: 0.252\n",
            "[9, 13800] Validation loss: 0.155\n",
            "[9, 14000] Training loss: 0.275\n",
            "[9, 14000] Validation loss: 0.178\n",
            "[9, 14200] Training loss: 0.278\n",
            "[9, 14200] Validation loss: 0.189\n",
            "[9, 14400] Training loss: 0.288\n",
            "[9, 14400] Validation loss: 0.371\n",
            "[9, 14600] Training loss: 0.271\n",
            "[9, 14600] Validation loss: 0.310\n",
            "[9, 14800] Training loss: 0.269\n",
            "[9, 14800] Validation loss: 0.486\n",
            "[9, 15000] Training loss: 0.266\n",
            "[9, 15000] Validation loss: 0.119\n",
            "[9, 15200] Training loss: 0.248\n",
            "[9, 15200] Validation loss: 0.307\n",
            "[9, 15400] Training loss: 0.270\n",
            "[9, 15400] Validation loss: 0.272\n",
            "[9, 15600] Training loss: 0.273\n",
            "[9, 15600] Validation loss: 0.303\n",
            "[9, 15800] Training loss: 0.275\n",
            "[9, 15800] Validation loss: 0.141\n",
            "[9, 16000] Training loss: 0.240\n",
            "[9, 16000] Validation loss: 0.384\n",
            "[9, 16200] Training loss: 0.284\n",
            "[9, 16200] Validation loss: 0.134\n",
            "[9, 16400] Training loss: 0.268\n",
            "[9, 16400] Validation loss: 0.372\n",
            "[9, 16600] Training loss: 0.262\n",
            "[9, 16600] Validation loss: 0.157\n",
            "[9, 16800] Training loss: 0.277\n",
            "[9, 16800] Validation loss: 0.099\n",
            "[9, 17000] Training loss: 0.245\n",
            "[9, 17000] Validation loss: 0.061\n",
            "[9, 17200] Training loss: 0.269\n",
            "[9, 17200] Validation loss: 0.196\n",
            "[9, 17400] Training loss: 0.261\n",
            "[9, 17400] Validation loss: 0.133\n",
            "[9, 17600] Training loss: 0.271\n",
            "[9, 17600] Validation loss: 0.180\n",
            "[9, 17800] Training loss: 0.271\n",
            "[9, 17800] Validation loss: 0.437\n",
            "[9, 18000] Training loss: 0.262\n",
            "[9, 18000] Validation loss: 0.296\n",
            "[9, 18200] Training loss: 0.256\n",
            "[9, 18200] Validation loss: 0.275\n",
            "[9, 18400] Training loss: 0.267\n",
            "[9, 18400] Validation loss: 0.499\n",
            "[9, 18600] Training loss: 0.266\n",
            "[9, 18600] Validation loss: 0.202\n",
            "[10,   200] Training loss: 0.259\n",
            "[10,   200] Validation loss: 0.108\n",
            "[10,   400] Training loss: 0.260\n",
            "[10,   400] Validation loss: 0.337\n",
            "[10,   600] Training loss: 0.279\n",
            "[10,   600] Validation loss: 0.121\n",
            "[10,   800] Training loss: 0.252\n",
            "[10,   800] Validation loss: 0.146\n",
            "[10,  1000] Training loss: 0.259\n",
            "[10,  1000] Validation loss: 0.171\n",
            "[10,  1200] Training loss: 0.261\n",
            "[10,  1200] Validation loss: 0.069\n",
            "[10,  1400] Training loss: 0.271\n",
            "[10,  1400] Validation loss: 0.311\n",
            "[10,  1600] Training loss: 0.266\n",
            "[10,  1600] Validation loss: 0.148\n",
            "[10,  1800] Training loss: 0.252\n",
            "[10,  1800] Validation loss: 0.123\n",
            "[10,  2000] Training loss: 0.249\n",
            "[10,  2000] Validation loss: 0.192\n",
            "[10,  2200] Training loss: 0.263\n",
            "[10,  2200] Validation loss: 0.423\n",
            "[10,  2400] Training loss: 0.263\n",
            "[10,  2400] Validation loss: 0.236\n",
            "[10,  2600] Training loss: 0.257\n",
            "[10,  2600] Validation loss: 0.370\n",
            "[10,  2800] Training loss: 0.258\n",
            "[10,  2800] Validation loss: 0.111\n",
            "[10,  3000] Training loss: 0.256\n",
            "[10,  3000] Validation loss: 0.278\n",
            "[10,  3200] Training loss: 0.267\n",
            "[10,  3200] Validation loss: 0.255\n",
            "[10,  3400] Training loss: 0.286\n",
            "[10,  3400] Validation loss: 0.201\n",
            "[10,  3600] Training loss: 0.263\n",
            "[10,  3600] Validation loss: 0.239\n",
            "[10,  3800] Training loss: 0.259\n",
            "[10,  3800] Validation loss: 0.215\n",
            "[10,  4000] Training loss: 0.279\n",
            "[10,  4000] Validation loss: 0.150\n",
            "[10,  4200] Training loss: 0.263\n",
            "[10,  4200] Validation loss: 0.297\n",
            "[10,  4400] Training loss: 0.270\n",
            "[10,  4400] Validation loss: 0.147\n",
            "[10,  4600] Training loss: 0.260\n",
            "[10,  4600] Validation loss: 0.331\n",
            "[10,  4800] Training loss: 0.256\n",
            "[10,  4800] Validation loss: 0.530\n",
            "[10,  5000] Training loss: 0.280\n",
            "[10,  5000] Validation loss: 0.164\n",
            "[10,  5200] Training loss: 0.274\n",
            "[10,  5200] Validation loss: 0.509\n",
            "[10,  5400] Training loss: 0.249\n",
            "[10,  5400] Validation loss: 0.199\n",
            "[10,  5600] Training loss: 0.261\n",
            "[10,  5600] Validation loss: 0.395\n",
            "[10,  5800] Training loss: 0.278\n",
            "[10,  5800] Validation loss: 0.126\n",
            "[10,  6000] Training loss: 0.252\n",
            "[10,  6000] Validation loss: 0.207\n",
            "[10,  6200] Training loss: 0.263\n",
            "[10,  6200] Validation loss: 0.083\n",
            "[10,  6400] Training loss: 0.274\n",
            "[10,  6400] Validation loss: 0.242\n",
            "[10,  6600] Training loss: 0.278\n",
            "[10,  6600] Validation loss: 0.202\n",
            "[10,  6800] Training loss: 0.258\n",
            "[10,  6800] Validation loss: 0.207\n",
            "[10,  7000] Training loss: 0.262\n",
            "[10,  7000] Validation loss: 0.092\n",
            "[10,  7200] Training loss: 0.263\n",
            "[10,  7200] Validation loss: 0.137\n",
            "[10,  7400] Training loss: 0.274\n",
            "[10,  7400] Validation loss: 0.422\n",
            "[10,  7600] Training loss: 0.249\n",
            "[10,  7600] Validation loss: 0.170\n",
            "[10,  7800] Training loss: 0.266\n",
            "[10,  7800] Validation loss: 0.385\n",
            "[10,  8000] Training loss: 0.274\n",
            "[10,  8000] Validation loss: 0.291\n",
            "[10,  8200] Training loss: 0.262\n",
            "[10,  8200] Validation loss: 0.214\n",
            "[10,  8400] Training loss: 0.294\n",
            "[10,  8400] Validation loss: 0.243\n",
            "[10,  8600] Training loss: 0.255\n",
            "[10,  8600] Validation loss: 0.194\n",
            "[10,  8800] Training loss: 0.250\n",
            "[10,  8800] Validation loss: 0.462\n",
            "[10,  9000] Training loss: 0.250\n",
            "[10,  9000] Validation loss: 0.222\n",
            "[10,  9200] Training loss: 0.250\n",
            "[10,  9200] Validation loss: 0.188\n",
            "[10,  9400] Training loss: 0.260\n",
            "[10,  9400] Validation loss: 0.248\n",
            "[10,  9600] Training loss: 0.264\n",
            "[10,  9600] Validation loss: 0.415\n",
            "[10,  9800] Training loss: 0.260\n",
            "[10,  9800] Validation loss: 0.247\n",
            "[10, 10000] Training loss: 0.260\n",
            "[10, 10000] Validation loss: 0.307\n",
            "[10, 10200] Training loss: 0.268\n",
            "[10, 10200] Validation loss: 0.095\n",
            "[10, 10400] Training loss: 0.238\n",
            "[10, 10400] Validation loss: 0.087\n",
            "[10, 10600] Training loss: 0.259\n",
            "[10, 10600] Validation loss: 0.025\n",
            "[10, 10800] Training loss: 0.250\n",
            "[10, 10800] Validation loss: 0.145\n",
            "[10, 11000] Training loss: 0.245\n",
            "[10, 11000] Validation loss: 0.181\n",
            "[10, 11200] Training loss: 0.250\n",
            "[10, 11200] Validation loss: 0.133\n",
            "[10, 11400] Training loss: 0.263\n",
            "[10, 11400] Validation loss: 0.166\n",
            "[10, 11600] Training loss: 0.253\n",
            "[10, 11600] Validation loss: 0.157\n",
            "[10, 11800] Training loss: 0.260\n",
            "[10, 11800] Validation loss: 0.333\n",
            "[10, 12000] Training loss: 0.267\n",
            "[10, 12000] Validation loss: 0.151\n",
            "[10, 12200] Training loss: 0.269\n",
            "[10, 12200] Validation loss: 0.265\n",
            "[10, 12400] Training loss: 0.269\n",
            "[10, 12400] Validation loss: 0.051\n",
            "[10, 12600] Training loss: 0.252\n",
            "[10, 12600] Validation loss: 0.185\n",
            "[10, 12800] Training loss: 0.278\n",
            "[10, 12800] Validation loss: 0.062\n",
            "[10, 13000] Training loss: 0.261\n",
            "[10, 13000] Validation loss: 0.337\n",
            "[10, 13200] Training loss: 0.249\n",
            "[10, 13200] Validation loss: 0.140\n",
            "[10, 13400] Training loss: 0.271\n",
            "[10, 13400] Validation loss: 0.346\n",
            "[10, 13600] Training loss: 0.258\n",
            "[10, 13600] Validation loss: 0.222\n",
            "[10, 13800] Training loss: 0.251\n",
            "[10, 13800] Validation loss: 0.230\n",
            "[10, 14000] Training loss: 0.258\n",
            "[10, 14000] Validation loss: 0.106\n",
            "[10, 14200] Training loss: 0.265\n",
            "[10, 14200] Validation loss: 0.194\n",
            "[10, 14400] Training loss: 0.247\n",
            "[10, 14400] Validation loss: 0.337\n",
            "[10, 14600] Training loss: 0.264\n",
            "[10, 14600] Validation loss: 0.115\n",
            "[10, 14800] Training loss: 0.262\n",
            "[10, 14800] Validation loss: 0.211\n",
            "[10, 15000] Training loss: 0.256\n",
            "[10, 15000] Validation loss: 0.039\n",
            "[10, 15200] Training loss: 0.271\n",
            "[10, 15200] Validation loss: 0.047\n",
            "[10, 15400] Training loss: 0.265\n",
            "[10, 15400] Validation loss: 0.178\n",
            "[10, 15600] Training loss: 0.237\n",
            "[10, 15600] Validation loss: 0.289\n",
            "[10, 15800] Training loss: 0.261\n",
            "[10, 15800] Validation loss: 0.369\n",
            "[10, 16000] Training loss: 0.260\n",
            "[10, 16000] Validation loss: 0.105\n",
            "[10, 16200] Training loss: 0.270\n",
            "[10, 16200] Validation loss: 0.342\n",
            "[10, 16400] Training loss: 0.269\n",
            "[10, 16400] Validation loss: 0.141\n",
            "[10, 16600] Training loss: 0.268\n",
            "[10, 16600] Validation loss: 0.038\n",
            "[10, 16800] Training loss: 0.254\n",
            "[10, 16800] Validation loss: 0.432\n",
            "[10, 17000] Training loss: 0.253\n",
            "[10, 17000] Validation loss: 0.081\n",
            "[10, 17200] Training loss: 0.254\n",
            "[10, 17200] Validation loss: 0.196\n",
            "[10, 17400] Training loss: 0.256\n",
            "[10, 17400] Validation loss: 0.341\n",
            "[10, 17600] Training loss: 0.262\n",
            "[10, 17600] Validation loss: 0.395\n",
            "[10, 17800] Training loss: 0.255\n",
            "[10, 17800] Validation loss: 0.145\n",
            "[10, 18000] Training loss: 0.258\n",
            "[10, 18000] Validation loss: 0.138\n",
            "[10, 18200] Training loss: 0.259\n",
            "[10, 18200] Validation loss: 0.236\n",
            "[10, 18400] Training loss: 0.268\n",
            "[10, 18400] Validation loss: 0.118\n",
            "[10, 18600] Training loss: 0.257\n",
            "[10, 18600] Validation loss: 0.305\n",
            "[11,   200] Training loss: 0.261\n",
            "[11,   200] Validation loss: 0.516\n",
            "[11,   400] Training loss: 0.262\n",
            "[11,   400] Validation loss: 0.164\n",
            "[11,   600] Training loss: 0.267\n",
            "[11,   600] Validation loss: 0.068\n",
            "[11,   800] Training loss: 0.247\n",
            "[11,   800] Validation loss: 0.229\n",
            "[11,  1000] Training loss: 0.263\n",
            "[11,  1000] Validation loss: 0.081\n",
            "[11,  1200] Training loss: 0.265\n",
            "[11,  1200] Validation loss: 0.227\n",
            "[11,  1400] Training loss: 0.265\n",
            "[11,  1400] Validation loss: 0.339\n",
            "[11,  1600] Training loss: 0.253\n",
            "[11,  1600] Validation loss: 0.056\n",
            "[11,  1800] Training loss: 0.253\n",
            "[11,  1800] Validation loss: 0.470\n",
            "[11,  2000] Training loss: 0.253\n",
            "[11,  2000] Validation loss: 0.401\n",
            "[11,  2200] Training loss: 0.261\n",
            "[11,  2200] Validation loss: 0.164\n",
            "[11,  2400] Training loss: 0.256\n",
            "[11,  2400] Validation loss: 0.090\n",
            "[11,  2600] Training loss: 0.249\n",
            "[11,  2600] Validation loss: 0.337\n",
            "[11,  2800] Training loss: 0.252\n",
            "[11,  2800] Validation loss: 0.366\n",
            "[11,  3000] Training loss: 0.271\n",
            "[11,  3000] Validation loss: 0.172\n",
            "[11,  3200] Training loss: 0.254\n",
            "[11,  3200] Validation loss: 0.464\n",
            "[11,  3400] Training loss: 0.260\n",
            "[11,  3400] Validation loss: 0.167\n",
            "[11,  3600] Training loss: 0.272\n",
            "[11,  3600] Validation loss: 0.357\n",
            "[11,  3800] Training loss: 0.274\n",
            "[11,  3800] Validation loss: 0.195\n",
            "[11,  4000] Training loss: 0.247\n",
            "[11,  4000] Validation loss: 0.390\n",
            "[11,  4200] Training loss: 0.238\n",
            "[11,  4200] Validation loss: 0.357\n",
            "[11,  4400] Training loss: 0.270\n",
            "[11,  4400] Validation loss: 0.722\n",
            "[11,  4600] Training loss: 0.239\n",
            "[11,  4600] Validation loss: 0.175\n",
            "[11,  4800] Training loss: 0.267\n",
            "[11,  4800] Validation loss: 0.210\n",
            "[11,  5000] Training loss: 0.274\n",
            "[11,  5000] Validation loss: 0.338\n",
            "[11,  5200] Training loss: 0.257\n",
            "[11,  5200] Validation loss: 0.215\n",
            "[11,  5400] Training loss: 0.246\n",
            "[11,  5400] Validation loss: 0.139\n",
            "[11,  5600] Training loss: 0.264\n",
            "[11,  5600] Validation loss: 0.244\n",
            "[11,  5800] Training loss: 0.255\n",
            "[11,  5800] Validation loss: 0.183\n",
            "[11,  6000] Training loss: 0.260\n",
            "[11,  6000] Validation loss: 0.222\n",
            "[11,  6200] Training loss: 0.261\n",
            "[11,  6200] Validation loss: 0.100\n",
            "[11,  6400] Training loss: 0.259\n",
            "[11,  6400] Validation loss: 0.253\n",
            "[11,  6600] Training loss: 0.246\n",
            "[11,  6600] Validation loss: 0.107\n",
            "[11,  6800] Training loss: 0.264\n",
            "[11,  6800] Validation loss: 0.133\n",
            "[11,  7000] Training loss: 0.255\n",
            "[11,  7000] Validation loss: 0.156\n",
            "[11,  7200] Training loss: 0.269\n",
            "[11,  7200] Validation loss: 0.123\n",
            "[11,  7400] Training loss: 0.241\n",
            "[11,  7400] Validation loss: 0.159\n",
            "[11,  7600] Training loss: 0.242\n",
            "[11,  7600] Validation loss: 0.218\n",
            "[11,  7800] Training loss: 0.260\n",
            "[11,  7800] Validation loss: 0.155\n",
            "[11,  8000] Training loss: 0.248\n",
            "[11,  8000] Validation loss: 0.627\n",
            "[11,  8200] Training loss: 0.254\n",
            "[11,  8200] Validation loss: 0.416\n",
            "[11,  8400] Training loss: 0.250\n",
            "[11,  8400] Validation loss: 0.073\n",
            "[11,  8600] Training loss: 0.255\n",
            "[11,  8600] Validation loss: 0.077\n",
            "[11,  8800] Training loss: 0.248\n",
            "[11,  8800] Validation loss: 0.451\n",
            "[11,  9000] Training loss: 0.249\n",
            "[11,  9000] Validation loss: 0.138\n",
            "[11,  9200] Training loss: 0.248\n",
            "[11,  9200] Validation loss: 0.272\n",
            "[11,  9400] Training loss: 0.254\n",
            "[11,  9400] Validation loss: 0.623\n",
            "[11,  9600] Training loss: 0.254\n",
            "[11,  9600] Validation loss: 0.259\n",
            "[11,  9800] Training loss: 0.260\n",
            "[11,  9800] Validation loss: 0.176\n",
            "[11, 10000] Training loss: 0.265\n",
            "[11, 10000] Validation loss: 0.266\n",
            "[11, 10200] Training loss: 0.261\n",
            "[11, 10200] Validation loss: 0.127\n",
            "[11, 10400] Training loss: 0.242\n",
            "[11, 10400] Validation loss: 0.101\n",
            "[11, 10600] Training loss: 0.262\n",
            "[11, 10600] Validation loss: 0.285\n",
            "[11, 10800] Training loss: 0.258\n",
            "[11, 10800] Validation loss: 0.350\n",
            "[11, 11000] Training loss: 0.258\n",
            "[11, 11000] Validation loss: 0.182\n",
            "[11, 11200] Training loss: 0.256\n",
            "[11, 11200] Validation loss: 0.217\n",
            "[11, 11400] Training loss: 0.245\n",
            "[11, 11400] Validation loss: 0.108\n",
            "[11, 11600] Training loss: 0.257\n",
            "[11, 11600] Validation loss: 0.269\n",
            "[11, 11800] Training loss: 0.243\n",
            "[11, 11800] Validation loss: 0.264\n",
            "[11, 12000] Training loss: 0.250\n",
            "[11, 12000] Validation loss: 0.130\n",
            "[11, 12200] Training loss: 0.239\n",
            "[11, 12200] Validation loss: 0.059\n",
            "[11, 12400] Training loss: 0.258\n",
            "[11, 12400] Validation loss: 0.284\n",
            "[11, 12600] Training loss: 0.265\n",
            "[11, 12600] Validation loss: 0.686\n",
            "[11, 12800] Training loss: 0.262\n",
            "[11, 12800] Validation loss: 0.458\n",
            "[11, 13000] Training loss: 0.252\n",
            "[11, 13000] Validation loss: 0.570\n",
            "[11, 13200] Training loss: 0.252\n",
            "[11, 13200] Validation loss: 0.397\n",
            "[11, 13400] Training loss: 0.272\n",
            "[11, 13400] Validation loss: 0.376\n",
            "[11, 13600] Training loss: 0.256\n",
            "[11, 13600] Validation loss: 0.285\n",
            "[11, 13800] Training loss: 0.254\n",
            "[11, 13800] Validation loss: 0.259\n",
            "[11, 14000] Training loss: 0.268\n",
            "[11, 14000] Validation loss: 0.029\n",
            "[11, 14200] Training loss: 0.252\n",
            "[11, 14200] Validation loss: 0.201\n",
            "[11, 14400] Training loss: 0.246\n",
            "[11, 14400] Validation loss: 0.100\n",
            "[11, 14600] Training loss: 0.247\n",
            "[11, 14600] Validation loss: 0.209\n",
            "[11, 14800] Training loss: 0.275\n",
            "[11, 14800] Validation loss: 0.192\n",
            "[11, 15000] Training loss: 0.264\n",
            "[11, 15000] Validation loss: 0.067\n",
            "[11, 15200] Training loss: 0.249\n",
            "[11, 15200] Validation loss: 0.509\n",
            "[11, 15400] Training loss: 0.262\n",
            "[11, 15400] Validation loss: 0.452\n",
            "[11, 15600] Training loss: 0.247\n",
            "[11, 15600] Validation loss: 0.109\n",
            "[11, 15800] Training loss: 0.246\n",
            "[11, 15800] Validation loss: 0.085\n",
            "[11, 16000] Training loss: 0.248\n",
            "[11, 16000] Validation loss: 0.062\n",
            "[11, 16200] Training loss: 0.234\n",
            "[11, 16200] Validation loss: 0.315\n",
            "[11, 16400] Training loss: 0.241\n",
            "[11, 16400] Validation loss: 0.594\n",
            "[11, 16600] Training loss: 0.249\n",
            "[11, 16600] Validation loss: 0.297\n",
            "[11, 16800] Training loss: 0.256\n",
            "[11, 16800] Validation loss: 0.083\n",
            "[11, 17000] Training loss: 0.264\n",
            "[11, 17000] Validation loss: 0.316\n",
            "[11, 17200] Training loss: 0.253\n",
            "[11, 17200] Validation loss: 0.163\n",
            "[11, 17400] Training loss: 0.264\n",
            "[11, 17400] Validation loss: 0.097\n",
            "[11, 17600] Training loss: 0.250\n",
            "[11, 17600] Validation loss: 0.241\n",
            "[11, 17800] Training loss: 0.252\n",
            "[11, 17800] Validation loss: 0.311\n",
            "[11, 18000] Training loss: 0.252\n",
            "[11, 18000] Validation loss: 0.263\n",
            "[11, 18200] Training loss: 0.258\n",
            "[11, 18200] Validation loss: 0.234\n",
            "[11, 18400] Training loss: 0.266\n",
            "[11, 18400] Validation loss: 0.157\n",
            "[11, 18600] Training loss: 0.248\n",
            "[11, 18600] Validation loss: 0.037\n",
            "[12,   200] Training loss: 0.251\n",
            "[12,   200] Validation loss: 0.134\n",
            "[12,   400] Training loss: 0.267\n",
            "[12,   400] Validation loss: 0.174\n",
            "[12,   600] Training loss: 0.246\n",
            "[12,   600] Validation loss: 0.149\n",
            "[12,   800] Training loss: 0.260\n",
            "[12,   800] Validation loss: 0.197\n",
            "[12,  1000] Training loss: 0.250\n",
            "[12,  1000] Validation loss: 0.173\n",
            "[12,  1200] Training loss: 0.235\n",
            "[12,  1200] Validation loss: 0.189\n",
            "[12,  1400] Training loss: 0.248\n",
            "[12,  1400] Validation loss: 0.264\n",
            "[12,  1600] Training loss: 0.252\n",
            "[12,  1600] Validation loss: 0.386\n",
            "[12,  1800] Training loss: 0.253\n",
            "[12,  1800] Validation loss: 0.178\n",
            "[12,  2000] Training loss: 0.263\n",
            "[12,  2000] Validation loss: 0.201\n",
            "[12,  2200] Training loss: 0.253\n",
            "[12,  2200] Validation loss: 0.371\n",
            "[12,  2400] Training loss: 0.237\n",
            "[12,  2400] Validation loss: 0.485\n",
            "[12,  2600] Training loss: 0.249\n",
            "[12,  2600] Validation loss: 0.092\n",
            "[12,  2800] Training loss: 0.242\n",
            "[12,  2800] Validation loss: 0.387\n",
            "[12,  3000] Training loss: 0.261\n",
            "[12,  3000] Validation loss: 0.694\n",
            "[12,  3200] Training loss: 0.235\n",
            "[12,  3200] Validation loss: 0.326\n",
            "[12,  3400] Training loss: 0.248\n",
            "[12,  3400] Validation loss: 0.703\n",
            "[12,  3600] Training loss: 0.265\n",
            "[12,  3600] Validation loss: 0.261\n",
            "[12,  3800] Training loss: 0.236\n",
            "[12,  3800] Validation loss: 0.249\n",
            "[12,  4000] Training loss: 0.263\n",
            "[12,  4000] Validation loss: 0.199\n",
            "[12,  4200] Training loss: 0.248\n",
            "[12,  4200] Validation loss: 0.464\n",
            "[12,  4400] Training loss: 0.246\n",
            "[12,  4400] Validation loss: 0.438\n",
            "[12,  4600] Training loss: 0.250\n",
            "[12,  4600] Validation loss: 0.464\n",
            "[12,  4800] Training loss: 0.252\n",
            "[12,  4800] Validation loss: 0.250\n",
            "[12,  5000] Training loss: 0.239\n",
            "[12,  5000] Validation loss: 0.198\n",
            "[12,  5200] Training loss: 0.256\n",
            "[12,  5200] Validation loss: 0.083\n",
            "[12,  5400] Training loss: 0.270\n",
            "[12,  5400] Validation loss: 0.460\n",
            "[12,  5600] Training loss: 0.237\n",
            "[12,  5600] Validation loss: 0.391\n",
            "[12,  5800] Training loss: 0.254\n",
            "[12,  5800] Validation loss: 0.076\n",
            "[12,  6000] Training loss: 0.252\n",
            "[12,  6000] Validation loss: 0.025\n",
            "[12,  6200] Training loss: 0.238\n",
            "[12,  6200] Validation loss: 0.219\n",
            "[12,  6400] Training loss: 0.245\n",
            "[12,  6400] Validation loss: 0.193\n",
            "[12,  6600] Training loss: 0.241\n",
            "[12,  6600] Validation loss: 0.316\n",
            "[12,  6800] Training loss: 0.252\n",
            "[12,  6800] Validation loss: 0.185\n",
            "[12,  7000] Training loss: 0.240\n",
            "[12,  7000] Validation loss: 0.073\n",
            "[12,  7200] Training loss: 0.265\n",
            "[12,  7200] Validation loss: 0.198\n",
            "[12,  7400] Training loss: 0.255\n",
            "[12,  7400] Validation loss: 0.075\n",
            "[12,  7600] Training loss: 0.235\n",
            "[12,  7600] Validation loss: 0.100\n",
            "[12,  7800] Training loss: 0.268\n",
            "[12,  7800] Validation loss: 0.347\n",
            "[12,  8000] Training loss: 0.261\n",
            "[12,  8000] Validation loss: 0.205\n",
            "[12,  8200] Training loss: 0.262\n",
            "[12,  8200] Validation loss: 0.171\n",
            "[12,  8400] Training loss: 0.254\n",
            "[12,  8400] Validation loss: 0.392\n",
            "[12,  8600] Training loss: 0.240\n",
            "[12,  8600] Validation loss: 0.175\n",
            "[12,  8800] Training loss: 0.247\n",
            "[12,  8800] Validation loss: 0.337\n",
            "[12,  9000] Training loss: 0.256\n",
            "[12,  9000] Validation loss: 0.305\n",
            "[12,  9200] Training loss: 0.254\n",
            "[12,  9200] Validation loss: 0.287\n",
            "[12,  9400] Training loss: 0.245\n",
            "[12,  9400] Validation loss: 0.376\n",
            "[12,  9600] Training loss: 0.273\n",
            "[12,  9600] Validation loss: 0.168\n",
            "[12,  9800] Training loss: 0.248\n",
            "[12,  9800] Validation loss: 0.389\n",
            "[12, 10000] Training loss: 0.245\n",
            "[12, 10000] Validation loss: 0.327\n",
            "[12, 10200] Training loss: 0.260\n",
            "[12, 10200] Validation loss: 0.197\n",
            "[12, 10400] Training loss: 0.252\n",
            "[12, 10400] Validation loss: 0.175\n",
            "[12, 10600] Training loss: 0.261\n",
            "[12, 10600] Validation loss: 0.076\n",
            "[12, 10800] Training loss: 0.242\n",
            "[12, 10800] Validation loss: 0.378\n",
            "[12, 11000] Training loss: 0.248\n",
            "[12, 11000] Validation loss: 0.280\n",
            "[12, 11200] Training loss: 0.265\n",
            "[12, 11200] Validation loss: 0.371\n",
            "[12, 11400] Training loss: 0.238\n",
            "[12, 11400] Validation loss: 0.339\n",
            "[12, 11600] Training loss: 0.254\n",
            "[12, 11600] Validation loss: 0.042\n",
            "[12, 11800] Training loss: 0.259\n",
            "[12, 11800] Validation loss: 0.411\n",
            "[12, 12000] Training loss: 0.238\n",
            "[12, 12000] Validation loss: 0.080\n",
            "[12, 12200] Training loss: 0.247\n",
            "[12, 12200] Validation loss: 0.102\n",
            "[12, 12400] Training loss: 0.267\n",
            "[12, 12400] Validation loss: 0.199\n",
            "[12, 12600] Training loss: 0.256\n",
            "[12, 12600] Validation loss: 0.219\n",
            "[12, 12800] Training loss: 0.241\n",
            "[12, 12800] Validation loss: 0.153\n",
            "[12, 13000] Training loss: 0.253\n",
            "[12, 13000] Validation loss: 0.058\n",
            "[12, 13200] Training loss: 0.267\n",
            "[12, 13200] Validation loss: 0.331\n",
            "[12, 13400] Training loss: 0.242\n",
            "[12, 13400] Validation loss: 0.272\n",
            "[12, 13600] Training loss: 0.252\n",
            "[12, 13600] Validation loss: 0.082\n",
            "[12, 13800] Training loss: 0.245\n",
            "[12, 13800] Validation loss: 0.301\n",
            "[12, 14000] Training loss: 0.252\n",
            "[12, 14000] Validation loss: 0.207\n",
            "[12, 14200] Training loss: 0.249\n",
            "[12, 14200] Validation loss: 0.086\n",
            "[12, 14400] Training loss: 0.223\n",
            "[12, 14400] Validation loss: 0.179\n",
            "[12, 14600] Training loss: 0.244\n",
            "[12, 14600] Validation loss: 0.050\n",
            "[12, 14800] Training loss: 0.260\n",
            "[12, 14800] Validation loss: 0.138\n",
            "[12, 15000] Training loss: 0.233\n",
            "[12, 15000] Validation loss: 0.120\n",
            "[12, 15200] Training loss: 0.255\n",
            "[12, 15200] Validation loss: 0.060\n",
            "[12, 15400] Training loss: 0.237\n",
            "[12, 15400] Validation loss: 0.062\n",
            "[12, 15600] Training loss: 0.247\n",
            "[12, 15600] Validation loss: 0.119\n",
            "[12, 15800] Training loss: 0.257\n",
            "[12, 15800] Validation loss: 0.545\n",
            "[12, 16000] Training loss: 0.258\n",
            "[12, 16000] Validation loss: 0.373\n",
            "[12, 16200] Training loss: 0.244\n",
            "[12, 16200] Validation loss: 0.079\n",
            "[12, 16400] Training loss: 0.261\n",
            "[12, 16400] Validation loss: 0.459\n",
            "[12, 16600] Training loss: 0.245\n",
            "[12, 16600] Validation loss: 0.069\n",
            "[12, 16800] Training loss: 0.260\n",
            "[12, 16800] Validation loss: 0.174\n",
            "[12, 17000] Training loss: 0.251\n",
            "[12, 17000] Validation loss: 0.292\n",
            "[12, 17200] Training loss: 0.240\n",
            "[12, 17200] Validation loss: 0.132\n",
            "[12, 17400] Training loss: 0.238\n",
            "[12, 17400] Validation loss: 0.484\n",
            "[12, 17600] Training loss: 0.255\n",
            "[12, 17600] Validation loss: 0.108\n",
            "[12, 17800] Training loss: 0.255\n",
            "[12, 17800] Validation loss: 0.345\n",
            "[12, 18000] Training loss: 0.250\n",
            "[12, 18000] Validation loss: 0.267\n",
            "[12, 18200] Training loss: 0.245\n",
            "[12, 18200] Validation loss: 0.159\n",
            "[12, 18400] Training loss: 0.247\n",
            "[12, 18400] Validation loss: 0.200\n",
            "[12, 18600] Training loss: 0.247\n",
            "[12, 18600] Validation loss: 0.167\n",
            "[13,   200] Training loss: 0.242\n",
            "[13,   200] Validation loss: 0.049\n",
            "[13,   400] Training loss: 0.255\n",
            "[13,   400] Validation loss: 0.235\n",
            "[13,   600] Training loss: 0.248\n",
            "[13,   600] Validation loss: 0.104\n",
            "[13,   800] Training loss: 0.235\n",
            "[13,   800] Validation loss: 0.228\n",
            "[13,  1000] Training loss: 0.242\n",
            "[13,  1000] Validation loss: 0.387\n",
            "[13,  1200] Training loss: 0.244\n",
            "[13,  1200] Validation loss: 0.354\n",
            "[13,  1400] Training loss: 0.243\n",
            "[13,  1400] Validation loss: 0.322\n",
            "[13,  1600] Training loss: 0.252\n",
            "[13,  1600] Validation loss: 0.138\n",
            "[13,  1800] Training loss: 0.262\n",
            "[13,  1800] Validation loss: 0.281\n",
            "[13,  2000] Training loss: 0.243\n",
            "[13,  2000] Validation loss: 0.218\n",
            "[13,  2200] Training loss: 0.221\n",
            "[13,  2200] Validation loss: 0.476\n",
            "[13,  2400] Training loss: 0.246\n",
            "[13,  2400] Validation loss: 0.240\n",
            "[13,  2600] Training loss: 0.235\n",
            "[13,  2600] Validation loss: 0.111\n",
            "[13,  2800] Training loss: 0.264\n",
            "[13,  2800] Validation loss: 0.235\n",
            "[13,  3000] Training loss: 0.243\n",
            "[13,  3000] Validation loss: 0.245\n",
            "[13,  3200] Training loss: 0.246\n",
            "[13,  3200] Validation loss: 0.190\n",
            "[13,  3400] Training loss: 0.241\n",
            "[13,  3400] Validation loss: 0.161\n",
            "[13,  3600] Training loss: 0.248\n",
            "[13,  3600] Validation loss: 0.270\n",
            "[13,  3800] Training loss: 0.248\n",
            "[13,  3800] Validation loss: 0.706\n",
            "[13,  4000] Training loss: 0.245\n",
            "[13,  4000] Validation loss: 0.090\n",
            "[13,  4200] Training loss: 0.239\n",
            "[13,  4200] Validation loss: 0.258\n",
            "[13,  4400] Training loss: 0.250\n",
            "[13,  4400] Validation loss: 0.461\n",
            "[13,  4600] Training loss: 0.248\n",
            "[13,  4600] Validation loss: 0.119\n",
            "[13,  4800] Training loss: 0.267\n",
            "[13,  4800] Validation loss: 0.065\n",
            "[13,  5000] Training loss: 0.247\n",
            "[13,  5000] Validation loss: 0.383\n",
            "[13,  5200] Training loss: 0.251\n",
            "[13,  5200] Validation loss: 0.202\n",
            "[13,  5400] Training loss: 0.253\n",
            "[13,  5400] Validation loss: 0.206\n",
            "[13,  5600] Training loss: 0.267\n",
            "[13,  5600] Validation loss: 0.337\n",
            "[13,  5800] Training loss: 0.254\n",
            "[13,  5800] Validation loss: 0.401\n",
            "[13,  6000] Training loss: 0.243\n",
            "[13,  6000] Validation loss: 0.390\n",
            "[13,  6200] Training loss: 0.241\n",
            "[13,  6200] Validation loss: 0.191\n",
            "[13,  6400] Training loss: 0.239\n",
            "[13,  6400] Validation loss: 0.379\n",
            "[13,  6600] Training loss: 0.240\n",
            "[13,  6600] Validation loss: 0.235\n",
            "[13,  6800] Training loss: 0.238\n",
            "[13,  6800] Validation loss: 0.259\n",
            "[13,  7000] Training loss: 0.246\n",
            "[13,  7000] Validation loss: 0.686\n",
            "[13,  7200] Training loss: 0.246\n",
            "[13,  7200] Validation loss: 0.127\n",
            "[13,  7400] Training loss: 0.216\n",
            "[13,  7400] Validation loss: 0.048\n",
            "[13,  7600] Training loss: 0.228\n",
            "[13,  7600] Validation loss: 0.239\n",
            "[13,  7800] Training loss: 0.264\n",
            "[13,  7800] Validation loss: 0.090\n",
            "[13,  8000] Training loss: 0.243\n",
            "[13,  8000] Validation loss: 0.172\n",
            "[13,  8200] Training loss: 0.243\n",
            "[13,  8200] Validation loss: 0.290\n",
            "[13,  8400] Training loss: 0.255\n",
            "[13,  8400] Validation loss: 0.226\n",
            "[13,  8600] Training loss: 0.232\n",
            "[13,  8600] Validation loss: 0.368\n",
            "[13,  8800] Training loss: 0.236\n",
            "[13,  8800] Validation loss: 0.254\n",
            "[13,  9000] Training loss: 0.247\n",
            "[13,  9000] Validation loss: 0.339\n",
            "[13,  9200] Training loss: 0.251\n",
            "[13,  9200] Validation loss: 0.572\n",
            "[13,  9400] Training loss: 0.238\n",
            "[13,  9400] Validation loss: 0.536\n",
            "[13,  9600] Training loss: 0.250\n",
            "[13,  9600] Validation loss: 0.131\n",
            "[13,  9800] Training loss: 0.256\n",
            "[13,  9800] Validation loss: 0.264\n",
            "[13, 10000] Training loss: 0.238\n",
            "[13, 10000] Validation loss: 0.255\n",
            "[13, 10200] Training loss: 0.233\n",
            "[13, 10200] Validation loss: 0.227\n",
            "[13, 10400] Training loss: 0.254\n",
            "[13, 10400] Validation loss: 0.189\n",
            "[13, 10600] Training loss: 0.256\n",
            "[13, 10600] Validation loss: 0.169\n",
            "[13, 10800] Training loss: 0.259\n",
            "[13, 10800] Validation loss: 0.333\n",
            "[13, 11000] Training loss: 0.252\n",
            "[13, 11000] Validation loss: 0.336\n",
            "[13, 11200] Training loss: 0.234\n",
            "[13, 11200] Validation loss: 0.192\n",
            "[13, 11400] Training loss: 0.239\n",
            "[13, 11400] Validation loss: 0.212\n",
            "[13, 11600] Training loss: 0.228\n",
            "[13, 11600] Validation loss: 0.028\n",
            "[13, 11800] Training loss: 0.237\n",
            "[13, 11800] Validation loss: 0.202\n",
            "[13, 12000] Training loss: 0.245\n",
            "[13, 12000] Validation loss: 0.219\n",
            "[13, 12200] Training loss: 0.252\n",
            "[13, 12200] Validation loss: 0.041\n",
            "[13, 12400] Training loss: 0.238\n",
            "[13, 12400] Validation loss: 0.389\n",
            "[13, 12600] Training loss: 0.241\n",
            "[13, 12600] Validation loss: 0.057\n",
            "[13, 12800] Training loss: 0.248\n",
            "[13, 12800] Validation loss: 0.385\n",
            "[13, 13000] Training loss: 0.250\n",
            "[13, 13000] Validation loss: 0.197\n",
            "[13, 13200] Training loss: 0.265\n",
            "[13, 13200] Validation loss: 0.223\n",
            "[13, 13400] Training loss: 0.241\n",
            "[13, 13400] Validation loss: 0.110\n",
            "[13, 13600] Training loss: 0.251\n",
            "[13, 13600] Validation loss: 0.342\n",
            "[13, 13800] Training loss: 0.249\n",
            "[13, 13800] Validation loss: 0.147\n",
            "[13, 14000] Training loss: 0.267\n",
            "[13, 14000] Validation loss: 0.197\n",
            "[13, 14200] Training loss: 0.247\n",
            "[13, 14200] Validation loss: 0.158\n",
            "[13, 14400] Training loss: 0.258\n",
            "[13, 14400] Validation loss: 0.303\n",
            "[13, 14600] Training loss: 0.246\n",
            "[13, 14600] Validation loss: 0.258\n",
            "[13, 14800] Training loss: 0.260\n",
            "[13, 14800] Validation loss: 0.192\n",
            "[13, 15000] Training loss: 0.249\n",
            "[13, 15000] Validation loss: 0.125\n",
            "[13, 15200] Training loss: 0.251\n",
            "[13, 15200] Validation loss: 0.149\n",
            "[13, 15400] Training loss: 0.252\n",
            "[13, 15400] Validation loss: 0.169\n",
            "[13, 15600] Training loss: 0.246\n",
            "[13, 15600] Validation loss: 0.427\n",
            "[13, 15800] Training loss: 0.242\n",
            "[13, 15800] Validation loss: 0.219\n",
            "[13, 16000] Training loss: 0.232\n",
            "[13, 16000] Validation loss: 0.158\n",
            "[13, 16200] Training loss: 0.251\n",
            "[13, 16200] Validation loss: 0.086\n",
            "[13, 16400] Training loss: 0.242\n",
            "[13, 16400] Validation loss: 0.338\n",
            "[13, 16600] Training loss: 0.251\n",
            "[13, 16600] Validation loss: 0.205\n",
            "[13, 16800] Training loss: 0.239\n",
            "[13, 16800] Validation loss: 0.106\n",
            "[13, 17000] Training loss: 0.240\n",
            "[13, 17000] Validation loss: 0.152\n",
            "[13, 17200] Training loss: 0.251\n",
            "[13, 17200] Validation loss: 0.120\n",
            "[13, 17400] Training loss: 0.260\n",
            "[13, 17400] Validation loss: 0.244\n",
            "[13, 17600] Training loss: 0.258\n",
            "[13, 17600] Validation loss: 0.277\n",
            "[13, 17800] Training loss: 0.225\n",
            "[13, 17800] Validation loss: 0.373\n",
            "[13, 18000] Training loss: 0.240\n",
            "[13, 18000] Validation loss: 0.332\n",
            "[13, 18200] Training loss: 0.242\n",
            "[13, 18200] Validation loss: 0.521\n",
            "[13, 18400] Training loss: 0.242\n",
            "[13, 18400] Validation loss: 0.228\n",
            "[13, 18600] Training loss: 0.246\n",
            "[13, 18600] Validation loss: 0.576\n",
            "[14,   200] Training loss: 0.239\n",
            "[14,   200] Validation loss: 0.199\n",
            "[14,   400] Training loss: 0.222\n",
            "[14,   400] Validation loss: 0.163\n",
            "[14,   600] Training loss: 0.254\n",
            "[14,   600] Validation loss: 0.062\n",
            "[14,   800] Training loss: 0.212\n",
            "[14,   800] Validation loss: 0.152\n",
            "[14,  1000] Training loss: 0.238\n",
            "[14,  1000] Validation loss: 0.139\n",
            "[14,  1200] Training loss: 0.242\n",
            "[14,  1200] Validation loss: 0.441\n",
            "[14,  1400] Training loss: 0.257\n",
            "[14,  1400] Validation loss: 0.205\n",
            "[14,  1600] Training loss: 0.229\n",
            "[14,  1600] Validation loss: 0.275\n",
            "[14,  1800] Training loss: 0.248\n",
            "[14,  1800] Validation loss: 0.194\n",
            "[14,  2000] Training loss: 0.236\n",
            "[14,  2000] Validation loss: 0.098\n",
            "[14,  2200] Training loss: 0.245\n",
            "[14,  2200] Validation loss: 0.464\n",
            "[14,  2400] Training loss: 0.224\n",
            "[14,  2400] Validation loss: 0.382\n",
            "[14,  2600] Training loss: 0.239\n",
            "[14,  2600] Validation loss: 0.418\n",
            "[14,  2800] Training loss: 0.242\n",
            "[14,  2800] Validation loss: 0.121\n",
            "[14,  3000] Training loss: 0.250\n",
            "[14,  3000] Validation loss: 0.092\n",
            "[14,  3200] Training loss: 0.253\n",
            "[14,  3200] Validation loss: 0.280\n",
            "[14,  3400] Training loss: 0.223\n",
            "[14,  3400] Validation loss: 0.214\n",
            "[14,  3600] Training loss: 0.229\n",
            "[14,  3600] Validation loss: 0.371\n",
            "[14,  3800] Training loss: 0.242\n",
            "[14,  3800] Validation loss: 0.069\n",
            "[14,  4000] Training loss: 0.237\n",
            "[14,  4000] Validation loss: 0.165\n",
            "[14,  4200] Training loss: 0.235\n",
            "[14,  4200] Validation loss: 0.092\n",
            "[14,  4400] Training loss: 0.242\n",
            "[14,  4400] Validation loss: 0.417\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ooU4g0RWHAxB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "training_examples = 6e5\n",
        "plot_every = 200 #batches\n",
        "training_loss_np = np.asarray(training_loss_list)\n",
        "validation_loss_np = np.asarray(validation_loss_list)\n",
        "\n",
        "x_axis = np.arange(0, len(training_loss_list))\n",
        "plt.plot(x_axis * bs * plot_every / training_examples, training_loss_np)\n",
        "plt.plot(x_axis * bs * plot_every / training_examples, validation_loss_np)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VYGMvzBa1mky",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **5. Network Testing**"
      ]
    },
    {
      "metadata": {
        "id": "OwiNwLg-d_Nz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GuiC_DNl1q_n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataiter = iter(test_loader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "print('GroundTruth: ', ' '.join('%5s' % class_name[labels[j]] for j in range(4)))\n",
        "outputs = net(images)\n",
        "_, predicted = torch.max(outputs, 1)\n",
        "\n",
        "print('Predicted: ', ' '.join('%5s' % class_name[predicted[j]] for j in range(4)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "djkk2I7SnB21",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's evaluate the model on the test data:\n",
        "\n",
        "While our densely-connected network we had a test accuracy of 97.8%, our basic convnet has a test accuracy of 99.3%: we decreased our error rate by 68% (relative)."
      ]
    },
    {
      "metadata": {
        "id": "PXPvUyOZpj8K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ACCURACY OF THE NETWORK\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z_hhvSIRpzvp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#WHICH CLASSESS PERFORMED BETTER\n",
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        c = (predicted == labels).squeeze()\n",
        "        for i in range(4):\n",
        "            label = labels[i]\n",
        "            class_correct[label] += c[i].item()\n",
        "            class_total[label] += 1\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "    print('Accuracy of %5s : %2d %%' % (\n",
        "        classes[i], 100 * class_correct[i] / class_total[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4RBL7aifncYn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Coses que falten:**\n",
        "\n",
        "\n",
        "1.   Passar-ho a la GPU (*en teoria ja est per repassar*)\n",
        "2.   Revisar hiperparametres/tamany xarxa/arquitectura per millorar el train\n",
        "3.   Afegir validation\n",
        "4.   Afegir test\n",
        "5.   Afegir plots de les losses, accuracy\n",
        "6.   Redactar tots els apartats\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}